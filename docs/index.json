[
  {
    "content": "Welcome to the SAP on Microsoft Labs This website is aimed to provide step-by-step guidance on how to prepare an SAP environment on Azure and demonstrate how can you integrate your SAP systems with Microsoft services and softwares.\nWe know that for an environment like SAP, having a robust infrasctructure is a must-have but with Microsoft you can do much more!\nThis website is divided in 3 main sections:\n Environment Setup: Where we will go thru the steps required to run the labs on the next sections. Please don’t skip. SAP on Microsoft Labs: Labs divided by category to create sample integrations between SAP and Microsoft Environment Cleanup: SAP can get quite expensive if let running 24x7. Here we will cleanup the environment created on Setup phase.  Each lab can be run individually from each other and are divided into sub-sections:\n Automation and Integration Data \u0026 AI Security Infrastructure and Management  On each lab introduction page, you will be presented a quickl introduction about the Microsoft technology involved, a video with the lab final result, estimated time for completion, and requirements.\nNavigating the site Navigation between sections can be accessed on the left menu and each one will be divided into smaller chunks of steps. Also on the top right of your screen you have the next/back buttons to allow for easier navigation.\nRequirements Before we start extending SAP, you need to have:\n An Azure subscription with enough credits for running the environment An Office365 subscription with access to Power Platform   Info Some components used will depend on Trial licenses that expire after a given period of time. Before building production content on top of those labs, make sure you do this in a properly licensed enviroment to avoid losing your work.\n   Warning ATTENTION: Some VMs for SAP HANA used in this lab can become quite expensive if you let it running 24x7. Make sure you define shutdown timers and in the end of your labs, go thru the Environment Cleanup section to remove unecessary resources. This lab is not responsible for costs related to its examples.\n  Running the Labs It is strongly recommended that you go thru Environment Setup before anything else once the Labs assume the standards deployed in the previous section. Each lab will have an introductiory page with the requirements, in case you want to jump right into a specific lab.\nAlso, follow the steps in the order they were presented otherwise you risk having to startover from scratch.\nIf you see something, say something This project is open source by nature and things can change pretty fast in the cloud environment. In case you see something wrong on the steps presented or something is not working as designed, feel free to contribute opening an issue or pushing a commit to our Github project page.\nAll set ! Let’s go ! Are you ready to get started?\nClick on the right arrow for next page or navigate using the left side menu.\n",
    "description": "",
    "tags": null,
    "title": "SAP on Microsoft",
    "uri": "/"
  },
  {
    "content": "SAP has a platform called SAP Cloud Application Library (SAP CAL), designed to automatically deploys SAP software on cloud hyperscalers.\nIn order to allow for SAP to deploy an S/4HANA environment in your Azure subscription, we need to setup permissions for it in your subscription.\nThis section will go thru the steps required to setup your subscription\nWhat you will need  An existing Azure subscription with enough credits for the labs Administrator access on the subscription A temporary text file to make note of the required information related to endpoints and keys   Warning In order to be able to deploy on your subscription, permissions used here will be quite broad. Make sure you go thru cleanup section to remove it after the labs, to avoid incidental usage of those credentials for non-authorized access to your subscription. In production environments, you should go thru the SAP and Microsoft best practices and setup access with the minimum required permissions and controls.\n  Expected duration This section is estimated to last no longer than 10 minutes\n",
    "description": "",
    "tags": null,
    "title": "Azure Setup for SAP CAL",
    "uri": "/env_setup/azure_setup/"
  },
  {
    "content": "Before we start extracting data from SAP, we need to create a repository to hold the extracted data.\nIn this example we will be using Azure Data Factory to extract the data and store it in a Blob Storage of a Datalake.\nThis section will show the steps required to prepare the environment for that:\n Log on to the Azure Portal and look for Storage Accounts  Create a new Storage Account and provide the required information. Select the same region that SAP was deployed:   Resource Group: SAP_ADF (create new RG) Storage Account Name: sapadfXXXX (must be unique on Azure, change XXXX by any set of 4 numbers) Region: East US Redundancy: LRS   On the newly created storage account, select Containers on the left pane and click on + Container  Name it cntsapadf and accept the default security of Private, once we don’t want to expose business data to the internet.   With those 4 steps we have created a repository to store the data.\nLet’s create the Azure Data Factory instance now:\n Log on to the Azure Portal and look for Data factories  Create a new Data factory and provide the required information. Select the same region that SAP was deployed:   Resource Group: SAP_ADF Name: sapadfXXXX (change XXXX by any set of 4 numbers) Region: East US   Click Next or select the tab Git configuration and check Configure Git later once we will not be storing our pipelines in a repository in this example. After this, click Create   Now we will configure the Integration Runtime on our Bastion Host to have access to the SAP data:\n On the Data Factory overview page, click on Open Azure Data Factory Studio  On the new tab that will open, create a New Pipeline  Name it adfsap_pipeline on the right tab that will open:  Click on the Toolbox icon on the left pane, then click on Integration runtimes and click on New. Select the Self Hosted option  On the Integration runtime setup step, select Self Hosted again  And name it SAPIntegrationRuntime; click Create  Once we are not performing those action on the Bastion host itself, we will choose Option 2: Manual Setup.   Copy the Key 1 Authentication Key that will be used on the runtime installation to some scratch area. Right click and Copy the URL from the Step 1 (we will use it to download the software on the bastion host)   Go to the Bastion Host via RDP and download the latest version, using the URL provided in the step #7:  Install the downloaded software:  On the setup phase, paste the Authentication Key from step #7, and click Register:  For this example, we will Enable remote access from intranet and click Next:  The Integration Runtime will contact the Data FActory using the Authentication Key provided and register itself. Click on Launch Configuration Manager and confirm that everything was OK:  All set on the Bastion Host, we will go back to the Azure Portal. Let’s make sure the Integration runtime is showing up on the Data Factory. Click on Refresh button and it should show the newly registered SAPIntegrationRuntime:   Now, finally, we are ready to start extracting data from SAP!\nThe next 2 steps will be similar, divided by data provider.\n",
    "description": "",
    "tags": null,
    "title": "Environment Preparation",
    "uri": "/labs/dataai/datafactory/step1/"
  },
  {
    "content": "In order to have SAP monitoring, we need to spin up an instance of Azure Monitoring.\nThis section will show the steps required for that to be accomplished:\n Log on to the Azure Portal and look for Azure Monitor for SAP  Create a new instance and provide the required information. Select the same region, vNet and subnet as SAP was deployed:   Region: East US vNET: SAPCALDefault-eastus Subnet: default   Do not worry about defining providers at this moment. Click on **Review + Create **  Once the deployment is complete Go to Resource and let’s start configuring the data Providers   Next steps will be divided by data provider.\n",
    "description": "",
    "tags": null,
    "title": "Setting up Azure Monitor for SAP",
    "uri": "/labs/inframgmt/monitoring/step1/"
  },
  {
    "content": "Before we start integrating SAP with Azure Active Directory for SSO, we need to setup SAP to allow this communication to happen.\nThis section will show the steps required for this to be accomplished:\nSAP Profile selection  Log on to the SAP BAstion host via Remote Desktop and look for open SAP GUI (SAP Logon) with user BPINST/Welcome1 Go to TCODE RZ10  We will change some parameters from the default SAP profile. Click on the Profile select DEFAULT, pick Extended maintenance and click on CHANGE  Click OK on the pop-up message   Setting up parameters We will change 3 parameters, following always the same procedure:\n Double-Click parameter login/ticket_only_by_https (default = 1)   Change its value to 0 and click on the Back button   Confirm the parameter change Repeat the same procedure to\n Parameter: icf/set_HTTPonly_flag_on_cookies / Value: 3  Parameter: icf/user_recheck / Value: 0     Double check changes and click Back Button:\n   login/ticket_only_by_https = 0 icf/set_HTTPonly_flag_on_cookies = 3 icf/user_recheck = 0    Save the Profile Changes   Click on the SAVE button and select No for errors   Confirm new profile activation   Click OK on the confirmation message   Reboot SAP In order for the changes become effective, we need to stop/start SAP. The simple way of doing this is going to the Virtual Machines under Azure Portal, selecting VMs SAP1 and SAP2 and clickgin Restart. It should take around 15 minutes for it to come back up, go grab a coffee. Confirming changes and activating parameters After the reboot, we need a final step which is activating the parameters to the Client 100 into SAP:\n Go to the SAP GUi on the Bastion Host and go to TCODE SICF_SESSIONS  Accept the message  Check the parameters changed before the reboot, select CLIENT 100 and click on ACTIVATE BUTTON   Alright ! This was the most complex part, changing SAP default profiel to accept SSO.\nOn the next steps we will configure Azure AD and integrate with SAP.\nCreate a new instance and provide the required information. Select the same region, vNet and subnet as SAP was deployed:   Region: East US vNET: SAPCALDefault-eastus Subnet: default   Do not worry about defining providers at this moment. Click on **Review + Create **  Once the deployment is complete Go to Resource and let’s start configuring the data Providers   Next steps will be divided by data provider.\n",
    "description": "",
    "tags": null,
    "title": "Setting up SAP",
    "uri": "/labs/security/adsso/step1/"
  },
  {
    "content": "Azure Data Factory In the world of big data, raw, unorganized data is often stored in relational, non-relational, and other storage systems. However, on its own, raw data doesn’t have the proper context or meaning to provide meaningful insights to analysts, data scientists, or business decision makers.\nBig data requires a service that can orchestrate and operationalize processes to refine these enormous stores of raw data into actionable business insights. Azure Data Factory is a managed cloud service that’s built for these complex hybrid extract-transform-load (ETL), extract-load-transform (ELT), and data integration projects.\nAzure Data Factory is Azure’s cloud ETL service for scale-out serverless data integration and data transformation. It offers a code-free UI for intuitive authoring and single-pane-of-glass monitoring and management.\nExtracting SAP DATA with Azure Data Factory You can copy data from SAP ECC and SAP BW to any supported sink data store. Specifically, the SAP ECC connector supports:\n Copying data from SAP ECC on SAP NetWeaver version 7.0 and later. Copying data from any objects exposed by SAP ECC OData services, such as:  SAP tables or views. Business Application Programming Interface [BAPI] objects. Data extractors. Data or intermediate documents (IDOCs) sent to SAP Process Integration (PI) that can be received as OData via relative adapters.   Copying data by using basic authentication.  What we will build   Estimated Time for this Lab This lab is estimated to take around 30 minutes.\nRequirements For this demo we will be using the following components from the Environment Setup:\n S/4HANA deployed from SAP CAL On-premises data gateway installed and configured Azure subscription  ",
    "description": "",
    "tags": null,
    "title": "Azure Data Factory",
    "uri": "/labs/dataai/datafactory/"
  },
  {
    "content": "Azure Monitor for SAP When you have critical applications and business processes relying on Azure resources, you’ll want to monitor those resources for their availability, performance, and operation.\nAzure Monitor for SAP Solutions is an Azure-native monitoring product for anyone running their SAP landscapes on Azure. With Azure Monitor for SAP Solutions, you can collect telemetry data from Azure infrastructure and databases in one central location and visually correlate the data for faster troubleshooting.\nYou can monitor different components of an SAP landscape, such as Azure virtual machines (VMs), high-availability cluster, SAP HANA database, SAP NetWeaver, and so on, by adding the corresponding provider for that component.\nOne of the benefits of monitoring SAP on Azure is the integrated view of both Infrastructure metrics (like CPU, RAM and Disk) alongside HANA database metrics (like memory usage and memory growth), and Netweaver metrics like concurrent sessions and locks. With embedded Log Analytics and Workbooks it allow for using the monitoring data so you can:\n Create custom visualizations by editing the default Workbooks provided by Azure Monitor for SAP Solutions. Write custom queries. Create custom alerts by using Azure Log Analytics workspace. Take advantage of the flexible retention period in Azure Monitor Logs/Log Analytics. Connect monitoring data with your ticketing system.  This allows for a rapid response in case of an incident response as well as providing a simpler and more integrated way of having the whole SAP performance picture in a glance.\nThis lab is based on the Azure Monitor for SAP Quickstart guide.\nWhat we will build   Estimated Time for this Lab This lab is estimated to take between 20 and 30 minutes.\nRequirements For this demo we will be using the following components from the Environment Setup:\n S/4HANA deployed from SAP CAL Azure subscription  ",
    "description": "",
    "tags": null,
    "title": "Monitoring SAP",
    "uri": "/labs/inframgmt/monitoring/"
  },
  {
    "content": "Azure Active Directory Single Sign On (SSO) Single sign-on is an authentication method that allows users to sign in using one set of credentials to multiple independent software systems. Using SSO means a user doesn’t have to sign in to every application they use. With SSO, users can access all needed applications without being required to authenticate using different credentials.\nAzure AD can be used to provide SSO capabilities to SAP, either on the cloud (BTP, SCP, Sucess Factors, among many other solutions) or on your on-premises environment, all while using the same corporate standards for security, like password complexity, Multi-Factor Authentication, Conditional Login, …\nThe general SSO flow can be seen below:\n User access URL protected by SSO. In the example we will use SAP Fiori SAP Fiori, redirects to the SSO provider (Azure) login screen User provides authentication details (username/password) from AD Credentials are generated and exchanged Authenticated users have credentials generated and sent to SAP Fiori SAP Fiori checks the credentials and allows access to itself  This lab is based on the Azure Active Directory single sign-on (SSO) integration with SAP Fiori tutorial.\nWhat we will build   Estimated Time for this Lab This lab is estimated to take between 60 minutes.\nRequirements For this demo we will be using the following components from the Environment Setup:\n S/4HANA deployed from SAP CAL Azure subscription  ",
    "description": "",
    "tags": null,
    "title": "Single Sign On",
    "uri": "/labs/security/adsso/"
  },
  {
    "content": "In this section we will create our Chatbot on Microsoft Power Platform.\nGo to Office 365, authenticate and select Power Apps. On Power Apps, expand Chatbots, select Create and click on the Basic conversational bot button. Fill the information to create the bot:\n Name: SAP Bot Language: Deisred language (examples in Brazilian Portuguese with translation) Environment: US   Once the bot is created, go to Topics and click on New Topic Let’s rename the newly created topic from Untitled to PO Details (en-US) / Detalhes do Pedido (pt-BR) The bot needs know which phrases trigger this topic, so we will add some examples of questions users may pose to accomplish what they need:\n en-US  I need details for a PO I want PO details I need to know line items of a purchase order Inform the products in a purchase order   pt-BR  Quero detalhes de uma ordem de compra Preciso detalhes de um pedido Quero saber os detalhes de uma PO Informar produtos em um pedido     Let’s add a new step, acknowledging the intent and asking the PO number to be queried, and another one asking the PO Number:\n Message - Acknowledge  en-US  Ok. If I understood it correctly you with to know the line items of a Purchase Order (PO). I will need some extra info for that.   pt-BR  Ok, entendi que você precisa de uma lista dos items de uma Ordem de Compra (PO). Para isso vou precisar do número do pedido.     Question  en-US  Can you inform the PO number? (exactly 10 chars - leading zeroes)   pt-BR  Você pdoeria informar o número do pedido? (exatamente 10 caracteres - zeros a esquerda)   Parameters:  Identify: User’s Entire Response Variable Name: PONumber Type: Text (SAP compares strings so that is why we have leading zeroes and exactly 10 chars) Usage: Bot (it allows us to use this info for further questions)       Again, let’s acknowledge the user input and add an action to query SAP.\n Message - Acknowledge  en-US  Thank you! Searching for order “xxx” details …   pt-BR  Obrigado! Pesquisando a ordem “xxx\"para você …   IMPORTANT: replace xxx with bot.PONumber from dynamic values, like example below   Add Action:  Add a Call an Action and click on Create a flow     Now on the Flow we will:\n Rename the Flow to GetSAPOrderItems Define an Input variable called PONumber (type Text) inside the flow. Later on we will map this to the Bot.PONumber parameter. We will show a table on the bot’s answer. For this we will add an Initialize Variable, call it OutputTable (Type String) and add the following markdown content (don’t forget to add new line at the end) that will be rendered as a table.  | Date | Item | Description | Quantity | Price | |-----------|:-----------:|:-----------:|:-----------:|-----------:|  Next we will invoke SAP BAPI method:\n Add a Call SAP function step Set the required parameters:  AS Host: SAP HANA public IP Client: 100 AS System Number: 00 SAP Function Name: BAPI_SALESORDER_GETSTATUS SALESDOCUMENT: PONumber variable from dynamic values     Let’s now teach the Flow how to interpret SAP’s response:\n Add a Parse JSON action Generate Schema based on the JSON sample below by clicking on Generate from Sample and pasting it. Once the Schema is generated, define Content parameter as STATUSINFO from Dynamic Values  [  {  \"DOC_NUMBER\": \"0000000728\",  \"DOC_DATE\": \"2018-11-06\",  \"PURCH_NO\": \"xcwer\",  \"PRC_STAT_H\": \"C\",  \"DLV_STAT_H\": \"C\",  \"REQ_DATE_H\": \"2018-11-06\",  \"DLV_BLOCK\": \"\",  \"ITM_NUMBER\": \"000010\",  \"MATERIAL\": \"CM-FL-V01\",  \"SHORT_TEXT\": \"Forklift\",  \"REQ_DATE\": \"2018-11-21\",  \"REQ_QTY\": \"1.000\",  \"CUM_CF_QTY\": \"1.000\",  \"SALES_UNIT\": \"ST\",  \"NET_VALUE\": \"8000.00\",  \"CURRENCY\": \"USD\",  \"NET_PRICE\": \"8000.00\",  \"COND_P_UNT\": \"1\",  \"COND_UNIT\": \"ST\",  \"DLV_STAT_I\": \"C\",  \"DELIV_NUMB\": \"\",  \"DELIV_ITEM\": \"000000\",  \"DELIV_DATE\": \"0000-00-00\",  \"DLV_QTY\": \"0.000\",  \"REF_QTY\": \"0.000\",  \"S_UNIT_ISO\": \"PCE\",  \"CD_UNT_ISO\": \"PCE\",  \"CURR_ISO\": \"USD\",  \"MATERIAL_EXTERNAL\": \"\",  \"MATERIAL_GUID\": \"\",  \"MATERIAL_VERSION\": \"\",  \"PO_ITM_NO\": \"\",  \"CREATION_DATE\": \"0000-00-00\",  \"CREATION_TIME\": \"00:00:00\",  \"S_UNIT_DLV\": \"\",  \"DLV_UNIT_ISO\": \"\",  \"REA_FOR_RE\": \"70\",  \"PURCH_NO_C\": \"xcwer\",  \"MATERIAL_LONG\": \"CM-FL-V01\"  },  {  \"DOC_NUMBER\": \"0000000728\",  \"DOC_DATE\": \"2018-11-06\",  \"PURCH_NO\": \"xcwer\",  \"PRC_STAT_H\": \"C\",  \"DLV_STAT_H\": \"C\",  \"REQ_DATE_H\": \"2018-11-06\",  \"DLV_BLOCK\": \"\",  \"ITM_NUMBER\": \"000020\",  \"MATERIAL\": \"CM-FL-V00\",  \"SHORT_TEXT\": \"Forklift\",  \"REQ_DATE\": \"2018-11-06\",  \"REQ_QTY\": \"7.000\",  \"CUM_CF_QTY\": \"0.000\",  \"SALES_UNIT\": \"ST\",  \"NET_VALUE\": \"58800.00\",  \"CURRENCY\": \"USD\",  \"NET_PRICE\": \"8400.00\",  \"COND_P_UNT\": \"1\",  \"COND_UNIT\": \"ST\",  \"DLV_STAT_I\": \"C\",  \"DELIV_NUMB\": \"\",  \"DELIV_ITEM\": \"000000\",  \"DELIV_DATE\": \"0000-00-00\",  \"DLV_QTY\": \"0.000\",  \"REF_QTY\": \"0.000\",  \"S_UNIT_ISO\": \"PCE\",  \"CD_UNT_ISO\": \"PCE\",  \"CURR_ISO\": \"USD\",  \"MATERIAL_EXTERNAL\": \"\",  \"MATERIAL_GUID\": \"\",  \"MATERIAL_VERSION\": \"\",  \"PO_ITM_NO\": \"\",  \"CREATION_DATE\": \"0000-00-00\",  \"CREATION_TIME\": \"00:00:00\",  \"S_UNIT_DLV\": \"\",  \"DLV_UNIT_ISO\": \"\",  \"REA_FOR_RE\": \"70\",  \"PURCH_NO_C\": \"xcwer\",  \"MATERIAL_LONG\": \"CM-FL-V00\"  }  ] As you can see, the response is an array, so we have to use Apply to Each action, based on the Body dynamic output of Parse JSON Inside the For Each loop, we will be appending the lines one by one to the outputTable varibale which had only the header so far. For each item in the array it will add a new line. We will add an Append to string variable action and use “|” to separate the dynamic values.\nIMPORTANT: Remember to include opening and closing “|” and Add a NEW LINE in the end\n Dynamic values used to build the line are: DOC_DATE, MATERIAL, SHORT_TEXT, REQ_QTY, NET_PRICE   The last step on the Flow is to return the OutputTable variable back to the bot. Add a Return value to Power Virtual Agents action after the For Each (not inside it) and create a variable called outputTable (Type String) which will have the dynamic value OutputTable Save the Flow and let’s go back to the bot’s topic itself.\nNow we should be able to see the newly created Flow GetSAPOrderItens when adding the Call an action Here we will link the bot’s variables to the Flow ones. Map PONumber to bot.PONumber. It should autmatically get the outputTable variable from the flow. Now we will show the outputTable variable (string) as part of the message. Add the text and use the outputTable dynamic value in the content. Bot will render it as table on Teams later on.\n en-US  Here are the order details outputTable   pt-BR  Aqui estão os detalhes do pedido outputTable     Finally, add an End action and SAVE\n",
    "description": "",
    "tags": null,
    "title": "Creating our Chatbot",
    "uri": "/labs/autoint/virtualagents/step1/"
  },
  {
    "content": "In this step we will create a Logic App on Azure Portal.\n Go to Logic Apps Click on Add Fill the required parameters as the picture below  Subscription and Resource Group Name: SAPDemo Region: East US (same one used on the previous steps) Plan type: Consumption     ",
    "description": "",
    "tags": null,
    "title": "Create a Logic App",
    "uri": "/labs/autoint/logicapps/step1/"
  },
  {
    "content": "Azure Logic Apps Azure Logic Apps is a cloud-based platform for creating and running automated workflows that integrate your apps, data, services, and systems. With this platform, you can quickly develop highly scalable integration solutions for your enterprise and business-to-business (B2B) scenarios. As a member of Azure Integration Services, Azure Logic Apps simplifies the way that you connect legacy, modern, and cutting-edge systems across cloud, on premises, and hybrid environments.\nThe following list describes just a few example tasks, business processes, and workloads that you can automate using the Azure Logic Apps service:\n  Schedule and send email notifications using Office 365 when a specific event happens, for example, an IDOC is received.\n  Route and process customer orders across on-premises systems and cloud services.\n  Move uploaded files from an SFTP or FTP server to Azure Storage and Datalakes.\n  All this can be accomplished with simple and Low Code-based flows that integrate Microsoft and non-Microsoft resources in a single, simple flow.\nTo securely access and run operations on various data sources, you can use managed connectors in your workflows. Choose from many hundreds of connectors in an abundant and growing Azure ecosystem, for example:\n  Azure services such as Blob Storage and Service Bus\n  Office 365 services such as Outlook, Excel, and SharePoint\n  Database servers such as SQL and Oracle\n  Enterprise systems such as SAP and IBM MQ\n  File shares such as FTP and SFTP\n  What we will build In this lab we will build an API and receives a JSON payload with the Sales Order to be queries on SAP and sends the required information via Outlook using native SAP and Office 365 integrations.\nWhenever this API is used, it triggers a Logic App (built with no code) that will analyze the payload, run a BAPI on SAP, process the response, build an HTML table and send it by email alongside with the BAPI JSON.\nYou can see a sample of the lab below:\n  Estimated Time for this Lab This lab is estimated to take between 30 and 60 minutes.\nRequirements For this demo we will be using the following components from the Environment Setup:\n S/4HANA deployed from SAP CAL On-premises data gateway installed and configured Azure subscription  ",
    "description": "",
    "tags": null,
    "title": "Azure Logic Apps",
    "uri": "/labs/autoint/logicapps/"
  },
  {
    "content": "Power Virtual Agent Microsoft Power Virtual Agents lets you create powerful chatbots that can answer questions posed by your customers, other employees, or visitors to your website or service.\nThese bots can be created easily without the need for data scientists or developers. Some of the ways that Power Virtual Agents bots have been used include:\n Sales help and support issues Product or Purchase Order informations Employee health and vacation benefits Common employee questions for businesses, products, processes  All this integrated with live SAP data, allowing customers to access the information they need, on a simple, common, practical platform they are used to, avoiding the burden of context switching between multiples apps to accomplish a task.\nPower Virtual Agents is available as both a standalone web app, and as a discrete app within Microsoft Teams.\nWhat we will build In this lab we will build an chatbot that answers Sales Order details, using Power Virtual Agents and publishing the bot to Microsoft Teams.\nWhenever this bot is invoked, it triggers a a flow that will call an SAP BAPI based on user inputs process the response, build an HTML table and send it back to the chatbot.\nYou can see a sample of the lab below:\n  Estimated Time for this Lab This lab is estimated to take between 30 and 60 minutes.\nRequirements For this demo we will be using the following components from the Environment Setup:\n S/4HANA deployed from SAP CAL On-premises data gateway installed and configured Azure subscription  ",
    "description": "",
    "tags": null,
    "title": "Power Virtual Agent",
    "uri": "/labs/autoint/virtualagents/"
  },
  {
    "content": "Azure Backup for HANA SAP HANA databases are critical workloads that require a low recovery-point objective (RPO) and long-term retention. You can back up SAP HANA databases running on Azure virtual machines (VMs) by using Azure Backup.\nAzure Backup is Azure’s native backup solution, which is BackInt certified by SAP. This offering aligns with Azure Backup’s mantra of zero-infrastructure backups, eliminating the need to deploy and manage backup infrastructure.\nBenefits:\n 15-minute Recovery Point Objective (RPO): Recovery of critical data of up to 15 minutes is possible. One-click, point-in-time restores: Easy to restore production data on SAP HANA databases to alternate servers. Chaining of backups and catalogs to perform restores is all managed by Azure behind the scenes. Long-term retention: For rigorous compliance and audit needs, you can retain your backups for years, based on the retention duration, beyond which the recovery points will be pruned automatically by the built-in lifecycle management capability. Backup Management from Azure: Use Azure Backup’s management and monitoring capabilities for improved management experience.  Sample Backup flow:\n  The scheduled backups are managed by crontab entries created on the HANA VM by Azure Backup, while the on-demand backups are directly triggered by the Azure Backup service.\n  Once SAP HANA Backup Engine/Backint receives the backup request, it prepares the SAP HANA database for a backup by creating a save point, and moving data to underlying storage volumes.\n  Backint then executes the read operation from the underlying data volumes – the index server and XS engine for the Tenant database and name server for the SYSTEMDB. Premium SSD disks can provide optimal I/O throughput for the backup streaming operation.\n  To stream the backup data, Backint creates up to three pipes, which directly write to Azure Backup’s Recovery Services vault.\n  Azure Backup attempts to achieve speeds up to 420 MB/sec for non-log backups and up to 100 MB/sec for log backups.\n  Detailed logs are written to the backup.log and backint.log files on the SAP HANA instance.\n  Once the backup streaming is complete, the catalog is streamed to the Recovery Services vault. If both the backup (full/differential/incremental/log) and the catalog for this backup are successfully streamed and saved into the Recovery Services vault, Azure Backup considers the backup operation is successful.\n  What we will build   Estimated Time for this Lab This lab is estimated to take between 90 and 120 minutes, mostly due to backup and restores processes.\nRequirements For this demo we will be using the following components from the Environment Setup:\n S/4HANA deployed from SAP CAL Azure subscription  ",
    "description": "",
    "tags": null,
    "title": "Azure Backup for HANA",
    "uri": "/labs/inframgmt/backup/"
  },
  {
    "content": "Microsoft Power Platform The Microsoft Power Platform is a powerful set of applications that allow you to automate processes, build solutions, analyze data, and create virtual agents. Microsoft Power Platform is more than the sum of its parts. When connected to Azure and hundreds of other apps, Microsoft Power Platform enables you to quickly deliver value and clear your app backlog. Below you can see some details for each solution that is part of the Power Platform:\n Power BI Power BI is a unified self-service and enterprise analytics solution that lets you visualize your data and share insights across your organization or embed them in your app or website. Azure analytics services and Power BI together allow you to quickly unlock insights for your entire organization. Power Apps Power Apps provides a low-code approach to rapidly build apps for any device while seamlessly working with Azure services through a rich extensibility model for professional developers. Power Virtual Agents Power Virtual Agents enables you to create intelligent no-code chatbots and easily integrate with existing services by calling customer workflows using Power Automate and extending bots with custom capabilities using Bot Framework Skills. Power Automate Power Automate is a no-code automation platform that allows you to leverage previously off-limits legacy systems. Quickly automate solutions through simple workflow automation or through robust process automation using RPA. Enterprise Connectors Besides having mechanisms to access data, Power Platform provides a rich set of connectors to enhance the reach of what your solutions can do.\nOn the following labs you will have the opportunity to build upon those technologies and natively integrating with SAP for user empowerment and processes optimizations.\n",
    "description": "",
    "tags": null,
    "title": "Automation and Integration",
    "uri": "/labs/autoint/"
  },
  {
    "content": "In this section we will install the required components for Azure and Microsoft be able to connect to your SAP environment.\nThis section is a summary of the guide Install data gateway and Connect to SAP systems.\nConnect to the Remote Desktop as on the previous step and using a browser on the Bastion Host, download 3 pieces of software (links might have changed, please check above guides for the most up-to-date links):\n .Net Framework latest version On-premises data gateway SAP Connector  Install the .Net Framework following the standard process. Reboot the Bastion Host.\nInstall the On-premises Data Gateway and configure it:\n Accept the default path and terms of use:  Once installation completes, we will register the gateway. Use the same email address of the Azure subscription. It will open a sign in window for you to complete authentication using your azure credentials.\n Select Register a new gateway:  Give it a name and define a recovery key:  Select the region. For this demos, East US is the prefered one:  Installation on the Data gateway on the Bastion Host side is complete and you should see a screen similar to the one below, with both PowerBI and PowerApps showing up as Ready:   Install the SAP .NET Connector and make sure you select Register WMI provider provider and install Assemblies to GAC Now, let’s move to the Azure part, creating the Gateway on the cloud to interface with the on-premises data gateway.\n",
    "description": "",
    "tags": null,
    "title": "Remote Desktop Setup",
    "uri": "/env_setup/datagwsetup/step1/"
  },
  {
    "content": "Creating an SAP CAL account First you need to create a free SAP Cloud Appliance Library (CAL) account.\nGo to https://cal.sap.com/ and click on Log On.\nOn the Login page, select Register and fill the required information.\nGranting Azure access Once you got your approval e-mail from SAP, proceed to the SAP CAL website and Log On with your credentials, then follow the steps below:\n Go to Accounts and click on Create Account  Give it a name, select Microsoft Azure and fill the data with the information gathered.  Click on Test Connection to make sure the parameters are valid.   Alright, now we have the proper access to deploy a S/4HANA in your subscription.\nDeploying SAP S/4HANA SAP CAL allows for a 30 day Trial of the Solutions available, where SAP licenses are waived; only the cloud provider hosting fees apply. You can setup auto-shutdown and auto-terminate later on to make sure we keep costs low.\nIn order to deploy a Trial S/4HANA in you subscription:\n  Go to Solutions, pick the latest SAP S/4HANA Fully-Activated Appliance and click on Create Instance.   Select the newly created Account   Read and Accept the License terms of the Trial   Go to Advanced Mode so we can understand the parameters used on the deploy   Select your account.   Give your instance a name, description and validate the networking settings for the VNet. SAP will create a Default SAP CAL Network in case you have a new subscription. Select Public Static IP address so we can have remote access without the need for a VPN.   We won’t be deploying Business Objects so on the next step, you can desselect this VM. Later on you can change the VM types on azure to cheaper, smaller or newer ones. Scroll down and check all the parameters, ports, disk sizes so you can familiarize with the solution.   Provide a password for SAP Admin access that will be customized during the deployment. There will be local pre-defined users and admin users with this password. For more details see the Getting Started Guide for S/4HANA on SAP CAL with the default users and passwords.   Here we can adjust when the SAP will be available:\n By Schedule - Scheduled start and shutdown Suspend on an Exact Date - Scheduled to be running for XX number of days and then shutdown. Manual - If you do not plan to use it everyday. When you activate you can setup a shutdown time in hours in case you forget.  On the right side you can see the costs changing depending on the selection   Click Create on the bottom of the page, review the data and go grab lunch (it should take 2-3 hours for the deployment to be complete).   You can monitor the progress of the deployment in the Instances tab. Wait for Activated status.   When the solution is deployed and activated, click on the instance name and check the top menu, where you can control the SAP landscape (suspend/terminate/activate) as well as the remote desktop IP address used as Bastion Host for managing the environment on Azure. You can also access the Getting Started Guide under Solution Info tab as well make sure all the VMs are up and SAP was started properly and is communicating. All done ! You have an SAP S/4HANA running in your susbcription !\nOn the next section let’s see how we can access it and key users/passwords/parameters for conection that will be required later on.\n",
    "description": "",
    "tags": null,
    "title": "SAP CAL Account Setup",
    "uri": "/env_setup/sap_cal/accsetup/"
  },
  {
    "content": "The SAP Cloud Appliance Library (CAL) offers a quick and easy way to consume the latest SAP solutions in the cloud, such as SAP S/4HANA, SAP HANA Express Edition, Industry Solutions etc.\nIt’s an online library of latest, preconfigured, ready-to-use SAP solutions that can be instantly deployed into your own public cloud accounts.\nIn this section we will create and account on SAP CAL, configure Azure permissions and deploy an S/4HANA environment on your subscription.\nWhat you will need  Azure subscription setup and Parameters/Values from previous step An existing SAP CAL account or create a new one following the steps  Expected duration This section is estimated to last around 2-3 hours, mostly depending on the SAP deployment itself.\nAdditional Links and Infos For future reference, part of this content was developed by taking information from:\nSAP Cloud Appliance Library on Microsoft Docs\n",
    "description": "",
    "tags": null,
    "title": "SAP Cloud Appliance Library (CAL)",
    "uri": "/env_setup/sap_cal/"
  },
  {
    "content": "First of all, we will need to create an Active Directory app registration that will be used by SAP CAL:\n Sign in to your Azure Account through the Azure Portal Select Azure Active Directory Select App Registrations  Select New Registration  Provide a name, accept defaults and select Register   So far this app registration can’t do much. Now let’s add a role to this application.\n",
    "description": "",
    "tags": null,
    "title": "Create and Azure Active Directory application",
    "uri": "/env_setup/azure_setup/azuread/"
  },
  {
    "content": "Before we start building our labs we need to have provisioned a few resources:\n An Azure subscription setup for SAP Cloud Appliance Library (SAP CAL) An existing S/4HANA environment or a trial deployed thru SAP CAL Installation of Microsoft Data Gateway components An account at the SAP Gateway Demo System for some integration examples  The following sections will guide you thru the steps to accomplish this and be ready to the labs.\n Info Some components used will depend on Trial licenses that expire after a given period of time. Before building production content on top of those labs, make sure you do this in a properly licensed enviroment to avoid losing your work.\n   Warning ATTENTION: Some VMs for SAP HANA used in this lab can become quite expensive if you let it running 24x7. Make sure you define shutdown timers and in the end of your labs, go thru the Environment Cleanup section to remove unecessary resources. This lab is not responsible for costs related to its examples.\n  ",
    "description": "",
    "tags": null,
    "title": "Environment Setup",
    "uri": "/env_setup/"
  },
  {
    "content": "Let’s create a new pipeline and extract data to the Blob Storage:\n On the Data Factory Studio, click on the Pencil icon on the left, click the Dataset Ellipsis and select New Dataset  For the datastore, we will search for SAP and select SAP HANA:  For the properties:   Name: SapHanaTable1 Create a New under Linked Service   Fill the required data for the linked service:   Name: SapHana1 Integration Runtime: SAPIntegrationRuntime Server Name: «HANA IP»:30215 Authentication Type: Basic User name: SAPHANADB Password: Password defined on the CAL setup phase Click on Test connection and if successful, click on Apply   Select the newly created SapHana1 linked service and click OK (we will not be selecting a table right now)   We are now communicating with SAP HANA. Let’s see some data from Materials:\n On the left pane, select the new Dataset created SapHanaTable1 and for Table select SAPHANADB.MATDOC. Click Preview Data. (it may take a while for it to discover all the existing tables)  You should see a sample of the table data. Take a look and close the box on the top right corner.   Alright, so by now we are communciating with SAP and have access to data. Let’s create the pipeline to extract it to Blob.\nJump to STEP 4 on this LAB: Extracting data (Step 3 will be the same we just did but accessing SAP by the Netweaver layer)\n",
    "description": "",
    "tags": null,
    "title": "HANA Provider",
    "uri": "/labs/dataai/datafactory/step2/"
  },
  {
    "content": "In order to have Azure Backup backing up SAP HANA databases, we need to setup the Azure Backup as well as the HANA Database Server.\nThis section will show the steps required for that to be accomplished:\nSAP HANA Setup We need to connect to the SAP HANA Server and run a script that will prepare the database for Azure Backup.\n Log on to the Azure Portal and open a Cloud Shell (BASH)  We need to SSH to the server, using the SAP provided certificate key (can be downloaded from SAP CAL again), previously uploaded to the CloudShell and having permission as 400).  mod@Azure:~$ ssh -i \u003cKEYNAME\u003e.pem root@\u003cHANA PUBLIC IP\u003e sid-hdb-s4h:~ #  Then we will change the user to the HANA administrator hdbadm, store the database password in a secure storage, and return to root.  sid-hdb-s4h:~ # sudo su - hdbadm sid-hdb-s4h:HDB:hdbadm /usr/sap/HDB/HDB02 2\u003e hdbuserstore set azure_key localhost:30213 SYSTEM '\u003cYOUR SAP DEFINED PASSWORD\u003e' sid-hdb-s4h:HDB:hdbadm /usr/sap/HDB/HDB02 4\u003e exit logout sid-hdb-s4h:~ #  As root, we will download and run the setup script:  sid-hdb-s4h:~ # wget https://go.microsoft.com/fwlink/?linkid=2173610 -O pre-script.sh --2022-03-14 19:44:01-- https://go.microsoft.com/fwlink/?linkid=2173610 Resolving go.microsoft.com (go.microsoft.com)... 184.50.50.164, 2600:1408:c400:e82::2c1a, 2600:1408:c400:e80::2c1a Connecting to go.microsoft.com (go.microsoft.com)|184.50.50.164|:443... connected. HTTP request sent, awaiting response... 302 Moved Temporarily Location: https://download.microsoft.com/download/B/2/E/B2E01EF8-C247-42A6-BCC7-E45B78F20C99/msawb-plugin-config-com-sap-hana.sh [following] --2022-03-14 19:44:01-- https://download.microsoft.com/download/B/2/E/B2E01EF8-C247-42A6-BCC7-E45B78F20C99/msawb-plugin-config-com-sap-hana.sh Resolving download.microsoft.com (download.microsoft.com)... 204.79.197.219 Connecting to download.microsoft.com (download.microsoft.com)|204.79.197.219|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 87552 (86K) [application/octet-stream] Saving to: ‘pre-script.sh’  sid-hdb-s4h:~ # chmod +x pre-script.sh sid-hdb-s4h:~ # ./pre-script.sh --system-key azure_key  [2022-03-14T19:45:08+00:00] [INFO] Checking if 'root'. [2022-03-14T19:45:08+00:00] [PASS] Running as 'root'. [2022-03-14T19:45:08+00:00] [INFO] Checking OS support. [2022-03-14T19:45:08+00:00] [PASS] Found supported OS_NAME_VERSION = 'SLES-15.1'. [2022-03-14T19:45:08+00:00] [INFO] Checking for free space in '/opt'. [2022-03-14T19:45:08+00:00] [PASS] Found at least 2 GiB space on '/opt'. [2022-03-14T19:45:08+00:00] [INFO] Checking HOSTNAMES. [2022-03-14T19:45:08+00:00] [PASS] Found HOSTNAMES = [ [2022-03-14T19:45:08+00:00] [INFO] '::1' [2022-03-14T19:45:08+00:00] [INFO] '10.0.0.166' [2022-03-14T19:45:08+00:00] [INFO] '127.0.0.1' [2022-03-14T19:45:08+00:00] [INFO] 'fe80::222:48ff:fe2e:bde7' ... [2022-03-15T17:56:40+00:00] [INFO] Checking login for BACKUP_KEY_USER = 'AZUREWLBACKUPHANAUSER'. [2022-03-15T17:56:40+00:00] [PASS] Checked login. [2022-03-15T17:56:40+00:00] [INFO] Granting privilege 'DATABASE ADMIN' to 'AZUREWLBACKUPHANAUSER'. [2022-03-15T17:56:40+00:00] [PASS] Granted privilege. [2022-03-15T17:56:40+00:00] [INFO] Granting privilege 'CATALOG READ' to 'AZUREWLBACKUPHANAUSER'. [2022-03-15T17:56:40+00:00] [PASS] Granted privilege. [2022-03-15T17:56:40+00:00] [INFO] Granting privilege 'INIFILE ADMIN' to 'AZUREWLBACKUPHANAUSER'. [2022-03-15T17:56:40+00:00] [PASS] Granted privilege. [2022-03-15T17:56:40+00:00] [INFO] Granting privilege 'BACKUP ADMIN' to 'AZUREWLBACKUPHANAUSER'. [2022-03-15T17:56:40+00:00] [PASS] Granted privilege. [2022-03-15T17:56:40+00:00] [INFO] Checking privilege 'CATALOG READ' on 'AZUREWLBACKUPHANAUSER'. [2022-03-15T17:56:40+00:00] [PASS] Checked privilege. [2022-03-15T17:56:40+00:00] [INFO] Checking privilege 'BACKUP ADMIN' on 'AZUREWLBACKUPHANAUSER'. [2022-03-15T17:56:41+00:00] [PASS] Checked privilege. [2022-03-15T17:56:41+00:00] [INFO] Checking privilege 'INIFILE ADMIN' on 'AZUREWLBACKUPHANAUSER'. [2022-03-15T17:56:41+00:00] [PASS] Checked privilege. [2022-03-15T17:56:41+00:00] [INFO] Checking privilege 'DATABASE ADMIN' on 'AZUREWLBACKUPHANAUSER'. [2022-03-15T17:56:41+00:00] [PASS] Checked privilege. [2022-03-15T17:56:41+00:00] [INFO] Adding user 'hdbadm' to group 'msawb'. [2022-03-15T17:56:41+00:00] [PASS] Successfully added user. [2022-03-15T17:56:41+00:00] [INFO] Writing to configuration. [2022-03-15T17:56:41+00:00] [PASS] Writing complete. [2022-03-15T17:56:41+00:00] [INFO] Writing to environment file. [2022-03-15T17:56:41+00:00] [PASS] Writing complete. [2022-03-15T17:56:41+00:00] [SUCC] Done. At this point our HANA database is ready to be accessed by Azure backup and Backint (SAP Native Backup Agent) has been configured to send data to Azure.\nAzure Backup Setup  Log on to the Azure Portal and open Backup Center  On Backup Center we will create a Vault to store the backup data  For Vault Type select Recovery Services Vault  Provide the Vault details:   Region: East US (Same as HANA database deployment)   Once the Vault is created, let’s configure the Backup targets inside the Vault.  Change the backup type to SAP HANA on Azure VM and click on Start Discovery.  Select the HANA VM «instance name-SAP1 and click on Discover DB at the bottom of the page  Azure will start a deployment for the agent. Wait until you see the sucessfull message on Notifications  Back to the Backup page, you should now have a View Details button. Click it so we can see the discovered Databases.  Here you can see the discovered databases. Check the info and then close then window to go back to Backup  Click on Configure Backup  You can accept the default policy, select an existing one or Edit the current policy.  In this lab we will create a simpler policy, called DemoPolicy. Click on Edit besides each item and configure it so it matches the example below.  Now let’s add the discovered HANA databases. Click in Add and on the window that will open, select the DBs.  Click on Enable Backup once you have selected the HDB and SYSTEMDB databases.  Now, let’s make sure the items were correctly added and prepare for the first backup. Click on Backup items and then SAP HANA in Azure VM  You should see a screen similar to this, with a Warning (initial backup pending) message   Congratulations, you have configured the HANA Backup using Azure Backup. Now let’s move to the next section and run our first manual backup.\n",
    "description": "",
    "tags": null,
    "title": "Setup Azure Backup",
    "uri": "/labs/inframgmt/backup/step1/"
  },
  {
    "content": "Let’s add a HANA Provider to collect metrics for us:\n On the Azure Monitor for SAP, go to Providers  Click Add  Let’s fill the required data:   Type: SAP HANA IP Address: SAP HANA Private IP Address Database tenant: SYSTEMDB SQL Port: 30215 Username: SAPHANADB Password: Password defined during deploy of SAP CAL   Wait for the Provider to be created and the data validated.  It takes about 10-15 minutes for information to be initially available on Azure Monitor. Once this time has passed, go to Monitoring -\u003e SAP HANA on the left blade.  Select the desired HANA instance (you can monitor several instances like DEV/QAS/PRD on so on…)   An from here on, you should be able to see collected data directly from SAP HANA:\nOverview with peak CPU and RAM, HANA services status and licenses) Historic utilization data for CPU and Memory Data size and growth And more data like Backups and System Checks for Save Points and Delta Merges Next step we will add operating system counter to the Azure Monitor for SAP.\n",
    "description": "",
    "tags": null,
    "title": "HANA Provider",
    "uri": "/labs/inframgmt/monitoring/step2/"
  },
  {
    "content": "Let’s add a HANA Provider to collect metrics for us:\nNode Exporter Agent Before we start, we need to add an agent to the monitored VM.\n Go to the Azure Portal and open a Cloud Shell. In the examples we used BASH as the interpreter.  Now we will upload the key that SAP CAL generated on the deployment so we can log on to the Linux OS. Click on the File Transfer icon and then Upload  Select the key which will be a *.pem file, downladed from SAP CAL. If you do not remember or cannot find the file, go to SAP CAL, select your instance, click on Download Key and download it again.  Let’s add permissions to the .PEM file uploaded on the Azure Cloud Shell (in this example the key is called demosap.pem)  chmod 400 demosap.pem With the key setup done, we can connect to the SAP HANA instance, usign the key as password for root:  ssh -i demosap.pem root@\u003cPUBLIC IP of HANA\u003e Let’s download the agent. Go to Prometheus Download Page and copy the address for the latest Node Exporter for Linux  Back to Azure Cloud Shell, we will download, extract the agent and copy it to /usr/bin.  sid-hdb-s4h:~ # wget https://github.com/prometheus/node_exporter/releases/download/v1.3.1/node_exporter-1.3.1.linux-amd64.tar.gz sid-hdb-s4h:~ # tar xvzf node_exporter-1.3.1.linux-amd64.tar.gz  node_exporter-1.3.1.linux-amd64/ node_exporter-1.3.1.linux-amd64/LICENSE node_exporter-1.3.1.linux-amd64/NOTICE node_exporter-1.3.1.linux-amd64/node_exporter sid-hdb-s4h:~ # cd node_exporter-1.3.1.linux-amd64/ sid-hdb-s4h:~/node_exporter-1.3.1.linux-amd64 # cp -pr node_exporter /usr/bin/ Let’s test the agent by invoking it:  sid-hdb-s4h:~ # node_exporter ts=2022-03-14T18:58:29.764Z caller=node_exporter.go:182 level=info msg=\"Starting node_exporter\" version=\"(version=1.3.1, branch=HEAD, revision=a2321e7b940ddcff26873612bccdf7cd4c42b6b6)\" ts=2022-03-14T18:58:29.764Z caller=node_exporter.go:183 level=info msg=\"Build context\" build_context=\"(go=go1.17.3, user=root@243aafa5525c, date=20211205-11:09:49)\" ts=2022-03-14T18:58:29.764Z caller=node_exporter.go:185 level=warn msg=\"Node Exporter is running as root user. This exporter is designed to run as unpriviledged user, root is not required.\" ts=2022-03-14T18:58:29.765Z caller=filesystem_common.go:111 level=info collector=filesystem msg=\"Parsed flag --collector.filesystem.mount-points-exclude\" flag=^/(dev|proc|run/credentials/.+|sys|var/lib/docker/.+)($|/) ts=2022-03-14T18:58:29.765Z caller=filesystem_common.go:113 level=info collector=filesystem msg=\"Parsed flag --collector.filesystem.fs-types-exclude\" flag=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$ ts=2022-03-14T18:58:29.765Z caller=node_exporter.go:108 level=info msg=\"Enabled collectors\" ts=2022-03-14T18:58:29.765Z caller=node_exporter.go:115 level=info collector=arp ts=2022-03-14T18:58:29.765Z caller=node_exporter.go:115 level=info collector=bcache On the Bastion Host, go to http://«SAP HANA Private IP»:9100/metrics  Kill the node_exporter process with CTRL+C and start it in background  nohup node_exporter \u0026 The agent is installed and communicating internally on the vNET. Let’s add the provider now\nLinux Provider  On the Azure Monitor for SAP, go to Providers  Click Add  Let’s fill the required data:   Type: OS Linux Endpoint: http://internal_ip_address:9100/metrics   Wait until the provider is created and give it 10-15 minutes for data to start to flow.  Go to Monitoring, select OS (Linux), and pick the host.   Congratulations, you just installed and finished the OS Linux Agent install.\nFrom here on, you should be able to see collected data directly from Linux:\nOverview with peak CPU and RAM, disk usage Historic CPU Monitoring Disk size and growth ",
    "description": "",
    "tags": null,
    "title": "Linux Provider",
    "uri": "/labs/inframgmt/monitoring/step3/"
  },
  {
    "content": "Now we will start to configure Azure Active Directory to be a provider for SAP. This will be a back and forth process where we configure AD, go to SAP to configure it, and return to finish AD configuration.\n Go to Azure Portal and select Azure Active Directory   Setting Up Active Directory In case you don’t have an AD configured already on your subscription, keep going. To use existing AD, jump to step #6.\nLet’s create and Azure AD Tenant. Click on Create and select Azure Active Directory; click Next to Configuration\n On the configuration tab, give it an organization name (in the example ADSAPDEMO) and a UNIQUE Domain (in the example we used the same name). Make sure the Tenant is in the same region that your S/4HANA (In the example United States). Click Review and Create  Click Create   Creating our AD user Go to Users on the AD Tenant left pane, and click on ADD +.  Select Create a User, with parameters:  Username: bpinst Name: SAP Admin User First Name: BP Last Name: INST Auto Generate Password Click show password and make note of the password. You will be prompted to change it on the first use.     Configuring SAP Fiori SSO Now we will go for the first part of the configuration, going to SAP and enabling SSO.\nGo to the SAP Bastion Host, with Remote Desktop, and open SAP GUI. Go to TCODE SAML2. A Browser window will open.  Logon user BPINST/Welcome1 to the Netweaver WEB. Make sure client = 100.  Click on Create SAML2.0 Local Provider  Set Provider name to http://S4H100 and lcick Next  Click Next again and Finish  Once creation is complete, it will show you the default parameters. Click on Edit and check Include Certificate on Signature. Click Save  Go to METADATA and select Download Metadata  It will download a file called metadata.xml that will be used to configure Azure AD   Configuring Azure AD app for SAP Fiori SSO On Azure portal, on the AD Tenant we created the user, click on Enterprise applications on the left panel and then Add +  Use SAP as filter and select SAP FIORI App. Accept the default name and click Create  Click on SAP Fiori under All Applications  Inside SAP Fiori APP in Azure AD, select Users and Groups on the left panel, followed by + Add User/Group  Let’s select the AD user we created to this app. In Users, click None Selected, pick SAP Admin User, click Select and Assign  Now we will start to configure the connection to SAP. On the left panel, select Single sign-on and click on SAML  Instead of filling all the parameters, we will ise the metadata.xml file SAP generated to us. Click on Upload metadata file and select the metadata.xml.  On the right panel that will open, you just need to configure the Sign on URL to https://vhcals4hcs.dummy.nodomain and click on Save.  Next we will tell to Azure AD, how to handle the SAP username conversions (Azure AD bpinst@XXXXXX.onmicrosoft.com to SAP user BPINST). Click on Edit under Attributes and Claims  Click on the Unique User Identifier Claim to change its default configuration  Setup the following parameters:  Source: Transformation Transformation: ExtractMailPrefix() Parameter 1: user.userprincipalname Click Add and Save    Check for the change and download the Base64 certificate (“SAP Fiori.cer”) and the Federation Metadata XML (“SAP Fiori.xml” file)   Configuring SAP Fiori SSO with Azure AD on SAP Now that we did the Azure AD part, we need to finally go back to SAP to finish configuring the trust between the two authentication providers.\n Go back to the Bastion Host, under the SAML configuration webpage. Click on the Trusted Providers tab, and Upload Metadata File from Azure AD.   Select the SAP Fiori.xml file from Azure AD. Click Next.   Select the SAP Fiori.cer certificate file from Azure AD. Click Next.   As an alias, use the name of the Azure AD tenant you used. Click Next   Change Digest Algorithm to SHA-256 and click Next   On Single Signon endpoints step, make HTTP POST is the default method. Click Next   Click Next until the last step (9) and click Finish   It will now show Azure AD as a Trusted provider. Click on Edit, go to Identity Federation and click Add. Select Unspecified and OK   Make sure parameters for Unspecified are as the picture below:   Again, click Add. Select E-mail and OK   Make sure parameters for E-mail are as the picture below and click Save   Enable the SSO in SAP Fiori:   Linking Azure AD user and SAP user As the last step, we will need to tell SAP which SAP users corresponds to the Azure AD user.\nIn this example we configured it so the Azure AD username (bpinst@XXXXX.onmicrosoft.com) must match SAP user’s email.\nLet’s go back to SAP GUI and configure it so we can test the SSO:\nOn SAP GUI, go to TCODE SU01  Select BPINST user and click the Pencil (change) icon  Scroll down the user properties under Address tab and include the Azure AD username (email) to the E-mail Address field. Click the Save icon (floppy disk)   PHEW ! That was a long one !\nNow we have setup the trust between SAP Fiori and Azure AD, and taught SAP how to match Azure AD users for authentication.\nLet’s go test it !\n",
    "description": "",
    "tags": null,
    "title": "Setup Azure AD, Fiori \u0026 SSO",
    "uri": "/labs/security/adsso/step2/"
  },
  {
    "content": "In this section we will test and debug our Chatbot on Microsoft Power Platform.\nOn the left side of the screen go to the Test bot and simulate a user conversation.\n IMPORTANT:  SAP expects a 10 char stirng with leading zeroes, so if you want to check order 728 you need to type 0000000728 Once we are doing a lab, we are not treating the input so type just the PO number with no other information, leading/trailing spaces, words, once we are passing the whole input to SAP. In a production bot more data treatment should be done, by creating a regexp and defining an Entity.     We should see the String we created on the Flow. Don’t worry with the format now, on Teams it will render as a table. If you need to debug the Flow, click on View flow details It will show all the flow runs and clicking on a run, will give you details step-by-step as well as more information for debug. Sucessful Run Example: Failed Run Example:\nIn this example I was logged on SAP GUI on exactly the required item, so I accidentally generated a lock that prevented the bot to query the data. Closing SAP GUI solved it. ",
    "description": "",
    "tags": null,
    "title": "Testing our Chatbot",
    "uri": "/labs/autoint/virtualagents/step2/"
  },
  {
    "content": "In this step we will create the Workflow on Azure Portal.\n Go to Logic Apps Click on your Logic App SAPDemo Go to Workflows and Add a new one as the parameters below:  Click on Designer and let’s start building the Workflow! As the first operation, which starts the flow, we will pick When a HTTP request is received  In order to customize the payload we expect, let’s Use a sample payload to generate schema  Sample payload: {\"id\": \"0000000728\"}    This will generate a schema for the trigger  Now we will create our temp variable to hold the desired part of the SAP reponse. Add a new action and type “initialize” on the search box. Select Initialize Variables  In this step, we will create na empty array to store the data:  Name: outputArray Type: Array Value: []    Next we will invoke BAPI method in SAP by adding a new Action (make sure you select Azure ) called [BAPI] Call method in SAP  Now we need to link this BAPI call with the On-premises Data Gateway we installed on Bastion Host. Click on Change connection  An now we provide the connection information required so Logic Apps can communicate with SAP:  Connection Name: SAPS4CAL DataGateway: Select your Gateway, in this case SapDemoGW Client: 100 Auth: Basic User: BPINST Password: Welcome1 AS HOST: Public IP of your SAP HANA on SAP CAL (this IP will change if you shutdown SAP. You will have to follow the same procedure to manually update upon activation or resort to dynamic DNS) AS SERVICE: 50000 AS SYSTEM NUMBER: 00 Accept other defaults and click Create    Logic App will test the connection and you should see the Connected status  Now let’s setup the BAPI method to be invoked  Business Object: BUS2032:SalesOrder Method: GETSTATUS:Display Sales Order:BAPI_SALESORDER_GETSTATUS Input: \u003cGETSTATUS xmlns=\"http://Microsoft.LobServices.Sap/2007/03/Rfc/\"\u003e\u003cSALESDOCUMENT\u003exxx\u003c/SALESDOCUMENT\u003e\u003c/GETSTATUS\u003e Replace xxx on the above Input with a Dynamic Content id extracted from the trigger on step 7. Make sure there are no spaces between SALESDOCUMENT    Add a new Parse JSON action  In content use JsonResponse that will be passed by BAPI  As we did on the initial step, select Use sample payload to generate schema and use the following JSON sample {\"STATUSINFO\":[{\"DOC_NUMBER\":\"0000000728\",\"DOC_DATE\":\"2018-11-06\",\"PURCH_NO\":\"xcwer\",\"PRC_STAT_H\":\"C\",\"DLV_STAT_H\":\"C\",\"REQ_DATE_H\":\"2018-11-06\",\"DLV_BLOCK\":\"\",\"ITM_NUMBER\":\"000010\",\"MATERIAL\":\"CM-FL-V01\",\"SHORT_TEXT\":\"Forklift\",\"REQ_DATE\":\"2018-11-21\",\"REQ_QTY\":1.0,\"CUM_CF_QTY\":1.0,\"SALES_UNIT\":\"ST\",\"NET_VALUE\":8000.0,\"CURRENCY\":\"USD\",\"NET_PRICE\":8000.0,\"COND_P_UNT\":1.0,\"COND_UNIT\":\"ST\",\"DLV_STAT_I\":\"C\",\"DELIV_NUMB\":\"\",\"DELIV_ITEM\":\"000000\",\"DELIV_DATE\":\"\",\"DLV_QTY\":0.0,\"REF_QTY\":0.0,\"S_UNIT_ISO\":\"PCE\",\"CD_UNT_ISO\":\"PCE\",\"CURR_ISO\":\"USD\",\"MATERIAL_EXTERNAL\":\"\",\"MATERIAL_GUID\":\"\",\"MATERIAL_VERSION\":\"\",\"PO_ITM_NO\":\"\",\"CREATION_DATE\":\"\",\"CREATION_TIME\":\"00:00:00\",\"S_UNIT_DLV\":\"\",\"DLV_UNIT_ISO\":\"\",\"REA_FOR_RE\":\"70\",\"PURCH_NO_C\":\"xcwer\",\"MATERIAL_LONG\":\"CM-FL-V01\"},{\"DOC_NUMBER\":\"0000000728\",\"DOC_DATE\":\"2018-11-06\",\"PURCH_NO\":\"xcwer\",\"PRC_STAT_H\":\"C\",\"DLV_STAT_H\":\"C\",\"REQ_DATE_H\":\"2018-11-06\",\"DLV_BLOCK\":\"\",\"ITM_NUMBER\":\"000020\",\"MATERIAL\":\"CM-FL-V00\",\"SHORT_TEXT\":\"Forklift\",\"REQ_DATE\":\"2018-11-06\",\"REQ_QTY\":7.0,\"CUM_CF_QTY\":0.0,\"SALES_UNIT\":\"ST\",\"NET_VALUE\":58800.0,\"CURRENCY\":\"USD\",\"NET_PRICE\":8400.0,\"COND_P_UNT\":1.0,\"COND_UNIT\":\"ST\",\"DLV_STAT_I\":\"C\",\"DELIV_NUMB\":\"\",\"DELIV_ITEM\":\"000000\",\"DELIV_DATE\":\"\",\"DLV_QTY\":0.0,\"REF_QTY\":0.0,\"S_UNIT_ISO\":\"PCE\",\"CD_UNT_ISO\":\"PCE\",\"CURR_ISO\":\"USD\",\"MATERIAL_EXTERNAL\":\"\",\"MATERIAL_GUID\":\"\",\"MATERIAL_VERSION\":\"\",\"PO_ITM_NO\":\"\",\"CREATION_DATE\":\"\",\"CREATION_TIME\":\"00:00:00\",\"S_UNIT_DLV\":\"\",\"DLV_UNIT_ISO\":\"\",\"REA_FOR_RE\":\"70\",\"PURCH_NO_C\":\"xcwer\",\"MATERIAL_LONG\":\"CM-FL-V00\"}]}  SAP Sales Order can have multiple Line Items, so we have to iterate in them. Add na For Each action and as parameter, STATUSINFO from Parse JSON previous action.  Inside the For Each loop add a Compose action. It will allow for us to pick desired fields for each line and build a JSON that suits our needs.  Build the desired JSON using the sample JSON below and replacing fields enclosed by \u003c\u003e using Dynamic Inputs as inputs (make sure you keep the commas) { \"Order\": \u003cDOC_NUMBER\u003e, \"Date\": \u003cDOC_DATE\u003e, \"Item\": \u003cMATERIAL\u003e, \"Description\": \u003cSHORT_TEXT\u003e, \"Quantity\": \u003cREQ_QTY\u003e, \"Price\": \u003cNET_PRICE\u003e }   Inside the For Each loop, after Compose, add na action Append to array variable. Select outputArray previously initialized as empty and for Value Outputs from Compose action  Once For Each runs and populates outputArray with the desired data, we will convert it to an HTML table. Add a Create HTML table Action and use the outputArray as the data for the table.  Finally, we will add an action Send an email from Office 365 Outlook. You may need to connect Logic Apps with Office 365, by clicking on Change connection as did previously for SAP.  Let’s now build the email template that will be used on every invocation  To: Destination email Subject: Details for order  (replace with dynamic content) Body: as picture below    As the API response we will add a Response action and return  Status Code: 200 Body: JsonResponse from Dynamic inputs    Don’t forget to click SAVE !!!! ;)   If everything wen’t fine, we are ready to test the lab we just built, with NO CODE at all !\n",
    "description": "",
    "tags": null,
    "title": "Create a Workflow",
    "uri": "/labs/autoint/logicapps/step2/"
  },
  {
    "content": "Chapter X Some Chapter title Lorem Ipsum.\n",
    "description": "",
    "tags": null,
    "title": "Data \u0026 AI",
    "uri": "/labs/dataai/"
  },
  {
    "content": "In this section we will create the endpoint on Azure for the installed on-premises data gateway.\nThis section is a summary of the guide Install data gateway and Connect to SAP systems.\nConnect to the Azure Portal and follow the steps below:\n Go to On-premises Data Gateways:  Select Add  Fill the required parameters. Make sure to Select the same region as used on the installation (East US) and Select the name of the gateway previously created that should be populated on the drop-down box.   Congratulations, you just setup communications between SAP and Azure in a secure and simple way.\nNow, let’s head to the SAP Gateway Demo System setup.\n",
    "description": "",
    "tags": null,
    "title": "Azure Data Gateway Setup",
    "uri": "/env_setup/datagwsetup/step2/"
  },
  {
    "content": "Once your SAP is deployed, you will need access to specific SAP tools in order to access it (like HANA Studio and SAP GUI). Those software come preinstalled on the Bastion Host deployed by SAP.\nLet’s connect to an RDP session on the Bastion Host.\nUse the Public IP for the Remote Desktop provided on the Instance tab.\n Default User: Administrator Default Password: The one used during the SAP deployment  Once connected you should see a screen similar to this. If the message says it is still under deployment, log off and wait a little bit longer. We will basically use 3 apps installed here:\n Web browser SAP Logon (GUI) HANA Studio  The connection information for the SAP GUI is:\n Client: 100 User: BPINST Password: Welcome1 SID: S4H   For SAP HANA Studio:\n Host: vhcalhdbdb (local hosts file) Instance Number: 02 Multiple Containers - Tenant Database: HDB Port: 30215 User: SAPHANADB Password: The one used during the SAP deployment  If not existant:\n Right click on the left pane and select Add System  Fill the connection info  Fill the user and password   Once connection is created, double click on the connection on the left pane to see the HANA status: Alright! SAP has been deployed, we are able to connect, and we are ready to start installing the Azure required components on the next section.\n",
    "description": "",
    "tags": null,
    "title": "Remote Desktop Access",
    "uri": "/env_setup/sap_cal/rdpacc/"
  },
  {
    "content": "In this section we will add permissions and a role to the app registrations, allowing it to perform actions on your Azure subscription on your behalf:\n Warning In order to be able to deploy on your subscription, permissions used here will be quite broad. Make sure you go thru cleanup section to remove it after the labs, to avoid incidental usage of those credentials for non-authorized access to your subscription. In production environments, you should go thru the SAP and Microsoft best practices and setup access with the minimum required permissions and controls.\n   Navigate to the level of scope you wish to assign the application to. For example, to assign a role at the subscription scope, select All services, General and Subscriptions.\n Select the particular subscription to assign the application to. Select Access control (IAM) Select Add role assignment  Select the role you wish to assign to the application. To allow the application to execute actions like reboot, start and stop instances, select the Contributor role.  On the Members tab, click on Select members and start typing the name of the App registration done on the previous step Select Review and Assign to finish assigning the role.  You will see your application in the list of users assigned to a role for that scope.\nYour service principal is set up. You can start using it to run your scripts or apps. The next section shows how to get values that are needed when signing in programmatically.\n",
    "description": "",
    "tags": null,
    "title": "Assign Role to Application",
    "uri": "/env_setup/azure_setup/adrole/"
  },
  {
    "content": "In order to allow for SAP CAL to access your subscription, you will need some parameters. This section will explain where to find them.\nCopy those values to a temp text document so you can easily copy and paste later on.\nGo to your Azure Portal and follow the steps below:\n Go to Subscriptions and copy the Subscription ID  Select Azure Active Directory. Select Properties. Copy Tenant ID.  Still on Azure Active Directory, go to App registrations and select your application. Copy Application (client) ID.  Select Certificates \u0026 secrets, then select New client secret.  Define a name for the secret and an expiration 3 Months. After adding the client secret, the value of the client secret is displayed. Copy this value now because you aren’t able to retrieve the client secret later. You will provide the client secret value (as application password) with the application ID to sign in as the application. If you missed this step, delete the existing secret and create a new one.  By that time you should have 4 pieces of info:\n Azure Subscription ID Tenant ID Application (client) ID Secret Value  Now we will go to SAP CAL to setup the Azure access to deploy SAP S/4HANA in your subscription.\n",
    "description": "",
    "tags": null,
    "title": "Get required information",
    "uri": "/env_setup/azure_setup/getinfo/"
  },
  {
    "content": "The following sections will be divided in different labs integrating several Microsoft services with SAP, such as:\n Automation and Integration  Azure Logic Apps Power Platform - Power Virtual Agents Power Platform - Power Automate Power Platform - Power Apps   Data \u0026 AI  Office 365 Integration Azure Synapse Power Platform - Power BI Azure Machine Learning   Security  Azure Sentinel Azure AD   Infra \u0026 Management  Azure Monitoring Azure Backup High-Availability    The following sections will guide you thru the steps to accomplish this and be ready to the labs.\n",
    "description": "",
    "tags": null,
    "title": "SAP on Microsoft Labs",
    "uri": "/labs/"
  },
  {
    "content": "Let’s create a new pipeline and extract data to the Blob Storage:\n On the Data Factory Studio, click on the Pencil icon on the left, click the Dataset Ellipsis and select New Dataset  For the datastore, we will search for SAP and select SAP Table:  For the properties:   Name: SapTable1 Create a New under Linked Service   Fill the required data for the linked service:   Name: SapTable1 Integration Runtime: SAPIntegrationRuntime Server Name: «HANA IP» System Number: 00 Client Id: 100 Username: BPINST Password: Welcome1 Click on Test connection and if successful, click on Apply   Select the newly created SapTable1 linked service, select Table MATDOC and click OK   We are now communicating with SAP via Netweaver.\nJump to STEP 4 on this LAB: Extracting data (this is based on the values provided on the Step 3, but pipeline source data can be changed to match what the dataset we just set up)\n",
    "description": "",
    "tags": null,
    "title": "SAP TABLE Provider",
    "uri": "/labs/dataai/datafactory/step3/"
  },
  {
    "content": "With Azure Backups configured and communicating with SAP HANA database, let’s execute a manual backup to test it.\nThis section will show the steps required for that to be accomplished:\n Log on to the Azure Portal and click on Backup items and then SAP HANA in Azure VM  You should see a screen similar to this, with a Warning (initial backup pending) message. Right+Click the elipsis and select Backup Now for both HDB and SYSTEMDB.  To monitor the jobs, click on Backup jobs and you will see both submitted jobs. They should be In Progress for now.  If you go to the Bastion Host and access HANA Studio, you can see the same jobs running from the SAP point of view. Double click Backup under HDB and see the right panel for details like estimated size, performance and status.  Double clicking on Backup for SYSTEMDB will bring another tab called Configuration. Here we can see the parameters set by the setup script we ran in the previous section. Backint is SAP’s backup agent for SAP, so having it configured means the agent is pushing data to Azure or other backup platform.  Wait until the backup is finished. In the example environment, it took 30 minutes to backup 251GB of data @ 142MB/s. Performance may vary depending on server usage and other conditions.  Same information can be seen on Azure Portal, by clicking on Backup Jobs and checking the status of the latest jobs:  By selecting Backup Items and checking the status SAP HANA Backups we will see SYSTEM and HDB. Click on View details for HDB.\n It will show the available point-in-time restore and backups made. Point-in-time restoers are a combination of Backups + Logs, allowing for the agent to reconstruct the database at a given point-in-time (In the picture below, we left it running during the night. In your environment you will see the green bar growing with time, depending of the Log Backups)   Congratulations, we are now ready to test the restore option. LEt’s move to the next section !\n",
    "description": "",
    "tags": null,
    "title": "Run a Backup",
    "uri": "/labs/inframgmt/backup/step2/"
  },
  {
    "content": "In this section we will publish our Chatbot to Teams and test it.\nOn the left side of the screen go to the Publish Publish the latest version. Once the bot is public, we will add it to our org’s Teams. Click on Go to Channels and then turn on Teams Channel After the publishing and channel activations succeeds, you should see the option to Open bot directly on Teams. On Teams, Add the Bot Run the same simulations as in the test bot. PO 0000000728 PO 0000001575 ",
    "description": "",
    "tags": null,
    "title": "Publishing to Teams",
    "uri": "/labs/autoint/virtualagents/step3/"
  },
  {
    "content": "In this step we will execute the Workflow created previously\nGo back to Overview, Enable Debug mode (will be used in the future) and copy the Workflow URL provided We need to invoke the API, so we can use an externall tool like Postman or use the embeded Run Trigger tool.\nFor Postman:\n Method: POST URL: copied on the previous step Body: raw - JSON Body content: {\"id\": \"0000000728\"} or {\"id\": \"0000001575\"}  Note: SAP compares strings so have that in mind with leading zeroes and 10 total chars   Hit Send.   For Run Trigger tool:\n Click on Run Trigger with payload  Fill the request:  Method: POST Content-Type: application/json Body content: {\"id\": \"0000000728\"} or {\"id\": \"0000001575\"}  Note: SAP compares strings so have that in mind with leading zeroes and 10 total chars   Click Run    When your workflow runs, you should receive an Output status = 200  You can see the run status on the Run History tab  And by selecting the desired Run, you can see step-by-step inputs and outputs of your workflow for debug.  For a sucessful Run you can also see the details and the time it took on every step:   Now go check your email inbox because you should have something similar to the one below: If you need to check or compare a Sales Order on SAP, go to the Bastion Host via Remote Desktop, open SAP GUI and follow the steps below:\n Logon with BPINST/Welcome1  Go to TCODE VA03 (Display Sales Orders)\nSearch for the order number used on VA03\n Order: 728 or 1575 (here SAP compared numbers and don’t care about leading zeroes) Hit Search and you should see the details (to return to the previous screen, use the green arrow by the TCODE)   Congratulations ! You just finished the first lab and was able to create an API that will query SAP Sales Order and send an email, all with ZERO LINES of CODE !\n",
    "description": "",
    "tags": null,
    "title": "Running the lab",
    "uri": "/labs/autoint/logicapps/step3/"
  },
  {
    "content": "Power Automate Power Automate is a service that helps you create automated workflows between your favorite apps and services to synchronize files, get notifications, collect data, and more.\nHere are a few examples of what you can do with Power Automate.\n Automate business processes Send automatic reminders for past due tasks Move business data between systems on a schedule Connect to more than 500 data sources or any publicly available API You can even automate tasks on your local computer like computing data in Excel.  Just think about time saved once you automate repetitive manual tasks simply by recording mouse clicks, keystrokes and copy paste steps from your desktop! Power Automate is all about automation.\nWhat skills do you need to have? Anyone from a basic business user to an IT professional can create automated processes using Power Automate’s no-code/low-code platform. Here you can see some templates of what can be built as well as Videos with demos around Power Automate\nPower Automate provides an extensive connector list so you can automate tasks using triggers and actions from multiple platforms, including SAP. What we will build In this lab we will build an automated flow (manually executed) that reads the contents of an Excel file stored on OneDrive and line by line creates new products on SAP by leveraging SAP Gateway ODATA interfaces.\nYou can see a sample of the lab below:\n  Estimated Time for this Lab This lab is estimated to take between 30 minutes.\nRequirements For this demo we will be using the following components from the Environment Setup:\n SAP Gateway Demo System Azure subscription  ",
    "description": "",
    "tags": null,
    "title": "Office365 Flow",
    "uri": "/labs/autoint/automate/office365flow/"
  },
  {
    "content": "Power Apps Power Apps is a suite of apps, services, and connectors, as well as a data platform, that provides a rapid development environment to build custom apps for your business needs. Using Power Apps, you can quickly build custom business apps that connect to your data stored either in the underlying data platform (Microsoft Dataverse) or in various online and on-premises data sources (such as SAP, SharePoint, Microsoft 365, Dynamics 365, SQL Server, and so on).\nApps built using Power Apps provide rich business logic and workflow capabilities to transform your manual business operations into digital, automated processes. What’s more, apps built using Power Apps have a responsive design and can run seamlessly in browser and on mobile devices (phone or tablet). Power Apps “democratizes” the business-app-building experience by enabling users to create feature-rich, custom business apps without writing code.\nWhat we will build In this lab we will build an API and receives a JSON payload with the Sales Order to be queries on SAP and sends the required information via Outlook using native SAP and Office 365 integrations.\nWhenever this API is used, it triggers a Logic App (built with no code) that will analyze the payload, run a BAPI on SAP, process the response, build an HTML table and send it by email alongside with the BAPI JSON.\nYou can see a sample of the lab below:\n  Estimated Time for this Lab This lab is estimated to take around 60 minutes.\nRequirements For this demo we will be using the following components from the Environment Setup:\n SAP Gateway Demo System Azure subscription  ",
    "description": "",
    "tags": null,
    "title": "Power Apps",
    "uri": "/labs/autoint/powerapps/"
  },
  {
    "content": "Power Automate What we will build   ",
    "description": "",
    "tags": null,
    "title": "Power Automate",
    "uri": "/labs/autoint/automate/"
  },
  {
    "content": "Chapter X Some Chapter title Lorem Ipsum.\n",
    "description": "",
    "tags": null,
    "title": "Security",
    "uri": "/labs/security/"
  },
  {
    "content": "Once SAP is installed and access is working, we need to install the required components to allow for Microsoft services to interact with SAP S/4HANA environments.\nThe on-premises data gateway acts as a bridge to provide quick and secure data transfer between on-premises data (data that isn’t in the cloud) and several Microsoft cloud services. These cloud services include Power BI, PowerApps, Power Automate, Azure Analysis Services, and Azure Logic Apps. By using a gateway, organizations can keep databases and other data sources on their on-premises networks, yet securely use that on-premises data in cloud services.\nThis section will go thru the steps required to get this software installed and accessing Azure resources.\nWhat you will need  Sucessfull deployment of S/4HANA and access validated in the previous step  Expected duration This section is estimated to last no longer than 30 minutes\nAdditional Links and Infos For future reference, part of this content was developed by taking information from:\nWhat is an On-premises Data Gateway\nInstall the On-premises Data Gateway\nUse the On-premises Data Gateway\nOn-premises Data Gateway setup guide\nSAP ERP Connectors\n",
    "description": "",
    "tags": null,
    "title": "Microsoft On-premises Data Gateway Installation",
    "uri": "/env_setup/datagwsetup/"
  },
  {
    "content": "Before we start sending data to SAP, we need to create an IoT Hub to receive the ioT telemetry data and process it as it arrives.\nThis section will show the steps required to prepare the environment for that:\nSetting up Azure IoT Hub  Log on to the Azure Portal and look for Iot Hub  On IoT Hub page, select Create  Provide the required information. Select the same region that SAP was deployed:   Resource Group: IotRG (create new RG) IoT Hub Name: SAPIOTLAB (must be unique on Azure) Region: East US Click: Next: Networking\u003e   Make sure we have Public Access setup so we can send data via internet. Click: Next: Management\u003e  For this lab, we can use the Free Tier to accomplish our goals. FRee Tiear can scale up to 8k messages/day. Click: Review + create  Check the values and click: Create   Creating our IoT Device For this lab we will use a virtual Raspberry Pi Emulator. It runs virtually the same code you can run on a physical device, making it simple for us to show the conectivity and data flow, without having to play with electronics. In the Newly created Iot Hub, select Devices on the left panel and click Add Device  Name it VirtualRapberryPi and make sure the defaults are like the picture. Click Create  Once created, you can see the device under the Devices panel. Click on the VirtualRaspberryPi device.  Here we will copy the Primary Connection String so the virtual device can communicate and authenticate with Azure.   Configuring the Raspberry Pi Azure IoT Simulator As described before we will use a virtual device to run this lab. This device is a very simple one with an LED and a temperature/humidity sensor that will measure and send to Azure IoT Hub the most recent data.\nGo to the Raspberry Pi Azure IoT Online Simulator. The app has 3 parts: The device diagram (1), the code (2) and the control section (3) where we can command the device and see the messages.  On the Code section, we need to provide the credentials for our IoT Hub. Replace the [Your IoT Hub device connection string] with the copied Primary Connection String. Make sure you leave the quotes around the Connection String.\n Make sure your lab is similar to the example and click Run to test it.  If everything is correct, you will see messages being sent to Azure IoT Hub. Message are in JSON format.  Back on the device page on Azure portal, click on Message to Device so we can send some data from the cloud to the device  Use the message template below and click Send Message {\"messageId\": 99, \"deviceId\": \"Raspberry Pi Web Client\", response: \"Hello from Iot Hub!\"}  Check back the Simulator console for the recently sent message:  In order to see the messages sent from the device, you can check the Overview on the left panel on the IoT Hub. Once we just started sending data, click on 1 Hour so we can zoom in the charts:\n To see the messages being sent, open an Cloud Shell from the top icon. Select BASH and run the following command to stream the data: az iot hub monitor-events --hub-name SAP IOTLAB   Congratulations ! You have setup the Azure IoT Hub and the Device is communicating properly.\nOn the next section we will build a Logic App that will consume this data and depending on the parameters received, send data to SAP.\n",
    "description": "",
    "tags": null,
    "title": "IoT Hub Setup",
    "uri": "/labs/iot/iot/step1/"
  },
  {
    "content": "  Info This is the same step performed on the Powerapps lab. In case you already created an SAP Custom Connector, you can skip to section #2 of the lab, otherwise, create the connector as per instructions below\n  In this step we will create a custom connector to SAP on Power Apps. Custom connectors are used when you have a specific data source for which you know how to handle the requests. In this example we will use a premade example for the SAP Gateway Demo System (configured on the setup phase, section 4 - in case you skipped this step, please stop and create your access to the SAP Gateway Demo).\nThe steps required are:\n Go to Office.com. Click on the 9 dots Icon on the Top Left Select Power Apps (it can also be found under All Apps)  In Power Apps, expand Data and click on Custom Connectors. Select New Connector and pick the Import from Github option  Select Custom, branch master and Connector SAP-ODATA-DEMO  Rename it to SAPGWDEMO, check the parameters and click on Security  Make sure we are using Basic Authentication and click Definition  This connector has already some preconfigured interfaces. Feel free to explore then to understand how it handles API calls and click on Create Connector to proceed.   Now you should have a custom connector configured, allowing you to access SAP Gateway ODATA APIs thru your Power Automate. On the next section we will create a Flow to use this data.\n",
    "description": "",
    "tags": null,
    "title": "SAP Custom Connector",
    "uri": "/labs/autoint/automate/office365flow/step1/"
  },
  {
    "content": "In this step we will create a custom connector to SAP on Power Apps. Custom connectors are used when you have a specific data source for which you know how to handle the requests. In this example we will use a premade example for the SAP Gateway Demo System (configured on the setup phase, section 4 - in case you skipped this step, please stop and create your access to the SAP Gateway Demo).\nThe steps required are:\n Go to Office.com. Click on the 9 dots Icon on the Top Left Select Power Apps (it can also be found under All Apps)  In Power Apps, expand Data and click on Custom Connectors. Select New Connector and pick the Import from Github option  Select Custom, branch master and Connector SAP-ODATA-DEMO  Rename it to SAPGWDEMO, check the parameters and click on Security  Make sure we are using Basic Authentication and click Definition  This connector has already some preconfigured interfaces. Feel free to explore then to understand how it handles API calls and click on Create Connector to proceed.   Now you should have a custom connector configured, allowing you to access SAP Gateway ODATA APIs thru your Power App. On the next section we will create an App to use this data.\n",
    "description": "",
    "tags": null,
    "title": "SAP Custom Connector",
    "uri": "/labs/autoint/powerapps/step1/"
  },
  {
    "content": "Let’s change some data and then run a restore for a point-in-time previous to the change.\nThis section will show the steps required for that to be accomplished:\nChanging some data  Go to the Bastion Host, and open the SAP GUI. Login with BPINST/Welcome1 default user.  On SAP GUI, go to TCODE MM02 to change a material.  For Material we will choose the CM-FL-V00 a Forklift. Click on the Checkmark.  For Plant we will choose the 1710 a Forklift. Click on the Checkmark.  Change the description from Forklift to something else. Click on SAVE   With the data changed, we now go to restore the database HDB to the previous state.\nRestoring the Database  Go to Backup items and select SAP HANA on Azure VM  Right+Click the ellipsis for HDB and select Restore  There will be 3 restore modes   Alternate Location - Restore to another HANA database on another host Overwrite DB - Restore on top of existing database Restore as Files - Restore to files on filesystem for manual recovery Select Overwrite DB and click on Select   Let’s pick 15-20 minutes prior to the time of change. In the example the change was 10:15, so we are selecting 9:59 as the point-in-time.  make sure the Restore Point is right and click OK  It will trigger a deployment.  To monitor the jobs, click on Backup jobs and you will see restore job. It should be In Progress for now. It should take roughly the same it took for backup. In this example 30 minutes.  In the meantime, if you check HANA Studio on BAstion host you will see HDB offline, being restored. REstore will take the database offline, restore it, and bring it back online.  After 30 minutes, the jobs is showing up as completed on the Azure Portal.   Now let’s check for the data restored\nChecking SAP status  Go to the Bastion Host and check HANA Studio for HDB being open. Double click for details.  Open SAP GUI. Login with BPINST/Welcome1 default user.  On SAP GUI, go to TCODE MM02 to change a material.  For Material we will choose the CM-FL-V00 a Forklift. Click on the Checkmark.  For Plant we will choose the 1710 a Forklift. Click on the Checkmark.  Description should be back to Forklift as it was before the change we did.   Congratulations, you have completed the Azure Backup for HANA Section !!!!\n",
    "description": "",
    "tags": null,
    "title": "Run a Restore",
    "uri": "/labs/inframgmt/backup/step3/"
  },
  {
    "content": "Chapter X Some Chapter title Lorem Ipsum.\n",
    "description": "",
    "tags": null,
    "title": "Infrastructure and Management",
    "uri": "/labs/inframgmt/"
  },
  {
    "content": "Azure IoT Hub The Internet of Things (IoT) is a network of physical devices that connect to and exchange data with other devices and services over the Internet or other network. There are currently over ten billion connected devices in the world and more are added every year. Anything that can be embedded with the necessary sensors and software can be connected over the internet.\nAzure IoT Hub is a managed service hosted in the cloud that acts as a central message hub for communication between an IoT application and its attached devices. You can connect millions of devices and their backend solutions reliably and securely. Almost any device can be connected to an IoT hub.\nIoT Hub scales to millions of simultaneously connected devices and millions of events per second to support your IoT workloads.\nYou can integrate IoT Hub with other Azure services to build complete, end-to-end solutions. For example, use:\n  Azure Event Grid to enable your business to react quickly to critical events in a reliable, scalable, and secure manner.\n  Azure Logic Apps to automate business processes.\n  Azure Machine Learning to add machine learning and AI models to your solution.\n  Azure Stream Analytics to run real-time analytic computations on the data streaming from your devices.\n  What we will build We will build a solution where a (virtual) IoT Device will send temperature and humidity data to Azure IoT Hub, which will trigger a Logic App.\nThis Logic app will process each device telemetry message and if temperatura is greater than 29 degrees Celsius, it will generate an IDOC and send it to SAP.\nThis could be used, for example, to mark a product defective or a batch of good produced marked for quality assurance.\n  Estimated Time for this Lab This lab is estimated to take between 60 minutes.\nRequirements For this demo we will be using the following components from the Environment Setup:\n S/4HANA deployed from SAP CAL Azure subscription  ",
    "description": "",
    "tags": null,
    "title": "Azure IoT \u0026 Logic Apps",
    "uri": "/labs/iot/iot/"
  },
  {
    "content": "In this step we will create a an App from scratch and consume data from SAP APIs via the Custom Conenctor from step #1.\nThe steps required are:\n Go to Office.com. Click on the 9 dots Icon on the Top Left Select Power Apps (it can also be found under All Apps)  In Power Apps, click on Create and select Black app  Select Blank Canvas App  Name it SAPDemoApp and select Tablet format. Click Create  Click Skip on the welcome message   Now we will start customizing our App:\n  Let’s add the Custom Connector as a Datasource. Click on the database icon on the left, Add Data, search for SAP and select SAPGWDEMO. It may require authentication; in this case use the credentials sent by SAP and password you changed in environment setup, section 4.   There are many ways of loading data. In this example we will create a button to do the initial loading. In a regular App, this would be automated upon App load, but for the sake of the example, we will keep it separate for now. Click on the + icon, drag and drop the Button component to the canvas, resize it and set the required parameters:\n Text = Reload Products Name = BtnLoad OnSelect = ClearCollect(zProducts, ‘SAPODATA[Sample]Connector’.ListProductSets().d.results)  The OnSelect property is the action we expect it to run when clicked. In this example we will be creating a data collection called zProducts and load it with results from the api call ListProductSet from SAP. You can ALT+CLICK or OPTION+CLICK the button to test it.   In order to display the Collection laoded, we will use a Gallery component that can have its layout customized. Click on the + icon, drag and drop the Vertical Gallery component to the canvas, resize it and set the required parameters:\n DataSoruce = zProducts Name = Gallery1 Layout = Title, subtitle and Body  By setting the parameters, it should display the zProducts content loaded from the button above (if not ALT+CLICK the button)   Let’s customize the fields displayed on Gallery1. Select the component Gallery1, click on Edit, and match the fields like the picture:\n Body1 = Price Subtitle2 = Category Title2 = Name     The gallery is designed to show part of the data. But we can have access to all data once we select and item. Click on the arrow and adjust the OnSelect property. By doing this, we will be loading a variable called zSelect with the content of the Gallery1 selected item.\n OnSelect = Set(zSelected, Gallery1.Selected)     Drag 7 x Text Label to the canvas, and adjust its Text Property as Name, Category, Supplier, Price, Product ID, Supplier ID, Description. Those will be static data labels. Then select those 7 labels, copy and paste then on the canvas and on the next step we will customize it to read values form zSelected.   For each of the pasted labels, customize the Text property as:\n Name = zSelected.Name Category = zSelected.Category Supplier = zSelected.SupplierName Price = zSelected.CurrencyCode\u0026\" “\u0026zSelected.Price Product ID = zSelected.ProductID Supplier ID = zSelected.SupplierID Description = zSelected.Description     Alright ! We have a functioning app consuming data from SAP Gateway and displaying data in a simple and easy way.\nYou can click on the play button on the top right to run the app On the next section we will start to add some funcionalities and call other APIs to augment the app.\n",
    "description": "",
    "tags": null,
    "title": "Create Power App",
    "uri": "/labs/autoint/powerapps/step2/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "IoT",
    "uri": "/labs/iot/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Power BI",
    "uri": "/labs/autoint/powerbi/"
  },
  {
    "content": "In this step we will setup an One Drive folder to run our lab.\nThe steps required are:\n Go to Office.com. Click on the 9 dots Icon on the Top Left Select One Drive (it can also be found under All Apps)  Under My Files. Select New and pick the Folder option.   Name: Excel2SAP   Download the sample spreadsheet. This is the product list we will automatically create into SAP. Open it and change the yellow cells following the template provided:   Category: Pick from the dropdown Name: *XXX-9999 Price: 0000-9999 The other fields are automated with formulas, so no need to change them.\n  Save and upload the Excel file to the Excel2SAP folder   Now you should have it all setup to build the Power Automate flow to insert this data into SAP via ODATA and SAP Gateway. On the next section we will create a Flow to use this data.\n",
    "description": "",
    "tags": null,
    "title": "Preparing OneDrive",
    "uri": "/labs/autoint/automate/office365flow/step2/"
  },
  {
    "content": "Let’s add some steps on our pipeline to extract data to the Blob Storage:\n On the Data Factory Studio, click on the adfsap_pipeline on the left, open the Move \u0026 Transform activity, and drag Copy data to the main panel:  For the Copy data activity, we will name it Hana2Blob, under General tab:  On the Source tab we will select the dataset we want to read data from. In this example the HANA one, but it could be the SAP Table one as well.  For the Sink (destination) dataset, we will create a new one:  We will select Azure Blob Storage as the dataset type:  As Output format, we will select DelimitedText for the sake of visualization. For datalakes you will probably use other formats like Parquet that are more space efficient.  Name the dataset HANAMATDOCS and click on New under Linked Services.  We will link this dataset with our Storage Account created on step #1. Fill the required data for the linked service:   Name: AzureBlobSAP Integration Runtime: AutoResolveIntegrationRuntime Authentication Method: Account Key Subscription: Select your current subscription Storage Account Name: sapadfXXXX Click on Test connection and if successful, click on Apply   Back on the properties screen, make sure the Linked service is AzureBlobSAP and that filepath is cntsapadf which is the name of the container we created on step #1. You can use the folder button and navigate to the container itself. Make sure other options match the picture:  Our simple pipeline, with one step, is ready. Let’s Publish it so we can run it. On the top of the page, click on Publish and on the right panel Publish again:   Let’s run the Pipeline manually for testing:\n On the Data Factory Studio, click on the adfsap_pipeline on the left, and select AddTrigger/Trigger now on the main panel:  You can monitor the run, by selecting the speed meter icon on the left, then Pipeline runs and refreshing for Status = Succeedded   Let’s see the output !\n On the Azure Portal, go back to the sapadfXXXX Storage Account you created on step #1. There should be a container called cntadfsap. Click on it.  Inside the container we can see a newly created txt file with ~28MB, named after the table we used to extract the data. Right click it and select Download  Open the downloaded file on your editor of choice (aka Visual Code :) ) and check the file contents:  If you used the SAP TABLE provider (step 3), you should see a file named MATDOC.txt. Follow steps 2 \u0026 3 and open this file instead.  Compare the extracted data with the content from HANA Studio, on the Bastion Host. Open HDB, go to Catalog, under SAPHANADB select Tables and run a query like ‘SELECT TOP 1000 FROM “SAPHANADB”.“MATDOC”’   Congratulations, you just learned how to integrate SAP with Azure Data Factory and include it in your pipelines.\n",
    "description": "",
    "tags": null,
    "title": "Extracting data",
    "uri": "/labs/dataai/datafactory/step4/"
  },
  {
    "content": "Now that you have done all the labs, it is important to cleanup deployed resources.\nBelow we will see the steps required for this process:\nSAP CAL Cleanup This process will remove all resources deployed by SAP into your Azure Subscription. This is a destructive process and in case you need to recover this environment, you can redeploy from scratch until the end of the SAP CAL Trial.\n Sign in to the SAP CAL and click on Log On. Go to Instances, click on the ellipsis button and select Terminate  Confirm the termination by clicking OK    Info This process can take up to 30 minutes. During the process the status will go from Active to Terminating. Once it is completed the status will change to Terminated\n  Wait until the termination is complete to proceed on the Azure part.\nAzure Cleanup This process will remove all resources under the deleted reesource groups of your Azure Subscription. This is a destructive process with no recovery possible besides redeploying.\n Warning Before proceeding, make sure SAP CAL resources have been removed and Instance status on SAP CAL is TERMINATED as per guidance above.\n   Sign in to your Azure Account through the Azure Portal\n  Let’s start by removing all the remaining resources deployed on the same SAP resource groups. Navigate to Resource Groups   Here you can see the existing resource groups.   Click on the ones you want to delete (the ones created by SAP have SAPCAL in their names) and click on Delete resource group. On the panel on the right side, mark Apply force delete and write the name of the Resrouce Group to confirm. Click Delete. REPEAT this process to all resource groups named SAPCAL or that you want to remove\n  Let’s remove the SAP CAL access from the Active Directory, thus removing its privileges to launch any new workloads on Azure. Go to Azure Active Directory   Go to App registrations and open the one you created for SAP CAL   Select Delete. Once completed SAP CAL credentials will be invalid.   SAP Gateway Demo System Cleanup Although not required, because it doesn’t impact in costs, you can opt to delete your SAP Gateway Demo Account. This process will remove the access and delete the account. In case you need to have access to it again, you will have to register again from scratch.\n Go to the ES5 Login page Click on the Delet Account button   All set All the resources have been deleted if the procedures were sucessfull.\nThank you for having followed thru the labs and we sincerely hope they were of value for you and your business!\nThis project is open source by nature and things can change pretty fast in the cloud environment.\nIn case you see something wrong on the steps presented, something is not working as designed, or you wanna give a feedback, feel free to contribute opening an issue or pushing a commit to our Github project page.\n",
    "description": "",
    "tags": null,
    "title": "Environment Cleanup",
    "uri": "/cleanup/"
  },
  {
    "content": "Now that the configuration steps are done, we are ready to test the SSO in action.\nAll the operations will be done on the Bastion Host once, even with internet access, name resolution and security certificates are generated using the internal network names by SAP CAL.\n From the Bastion Host, open an internet browser window and navigate to https://vhcals4hcs.dummy.nodomain:44301/sap/bc/ui2/flp. Confirm that it no longer asks for User and password, instead offering a choice of identity provider, in this case ADSAPDEMO as we specified on Azure AD. Click Continue.  You should be redirected to the Microsoft AD portal. Provide the Azure AD email address as created on the previous steps.  Use the initial password provided by Azure AD (and copied to a scratch area in the previous steps) if this is your first logon  If this is your first logon with this user, Azure AD will ask you to change your password.  Once logon is sucessfull, you will be redirected by Azure AD to the SAP Fiori page. In the background, Azure AD exchanged security keys and certificates with SAP Fiori that now recognizes this access as a valid SAP user.  Once SAP Fiori has launched, click on the top right icon, and make sure it is logged on using the right SAP user BPINST   Congratulations ! You have enabled SSO between SAP Fiori and Microsoft Azure AD and now users can log in to SAP using their corporate credentials, MFA, and any other security rules defined by your organization.\n",
    "description": "",
    "tags": null,
    "title": "Testing SSO",
    "uri": "/labs/security/adsso/step3/"
  },
  {
    "content": "Chapter X Some Chapter title Lorem Ipsum.\n",
    "description": "",
    "tags": null,
    "title": "AI Builder",
    "uri": "/labs/autoint/aibuilder/"
  },
  {
    "content": "In this step we will create the automation required to create products automatically on SAP via ODATA.\nThe steps required are:\n Go to Office.com. Click on the 9 dots Icon on the Top Left Select Power Apps (it can also be found under All Apps)  Go to Flows. Select New flow and pick the Instant cloud flow option.  Select the flow properties:   Name: Excel2SAPGateway Trigger: Manually trigger a flow Click Create   On the newly created flow, click on the New Step button to add a new step.  Filter by Excel Online, pick Excel Online (Business) and in Actions List rows present in a table (make sure you pick the Business version so it works with your Ofice365)  Fill the parameters for the Excel file to be used as input:   Location: OnceDrive for Business Document library: OneDrive File: Navigate to /Excel2SAP/NewProductList.xlsx Table: NEWPRODUCTS Click on New Step   Let’s get the SAP credentials for the SAP gateway. Go to Custom and pick the SAPGWDEMO connector created on step #1.  Under Actions, pick Get product  In the Get product Action, fill the required fields:   id: HT-1002 (must be an existing product under SAP Fiori ES5 Demo System) x-csrf-token: fetch (we will query and store the token for access later on)   Now we are ready to scan the Excel file and line by line act upon the data. Add a new step, under Control called Apply to each.  Pick value from Dynamic content, under List rows present in a table. Add a new action.  Under Custom, pick SAPGWDEMO connector (inside the Apply to each loop)  Pick the Add product action  Fill the x-csrf-token with the Dynamic content from Get product  Fill x-ms-cookie-header with the Expression below, and click OK: replace(outputs('Get_product')['headers']['Set-Cookie'],',',';')  Fill the remaining parameters with the values:   ProductID: Dynamic Content Product ID from List rows present in a table Type Code: Product (fixed value) Category: Dynamic Content Product ID from List rows present in a table Name: Dynamic Content Product ID from List rows present in a table SupplierID: 0100000046 (fixed value) Product Tax Code: 1 (fixed value) Currency Code: United States Dollar (fixed value) Price: Dynamic Content Price(USD) from List rows present in a table Description: Dynamic Content Description from List rows present in a table Unit of Measure: each (fixed value)   Our flow is ready ! Click SAVE   Now you should have it all setup to test the Power Automate flow.\nOn the next section we will execute the test and see the data on SAP.\n",
    "description": "",
    "tags": null,
    "title": "Building the Flow",
    "uri": "/labs/autoint/automate/office365flow/step3/"
  },
  {
    "content": "Let’s build a Logic App to consume the data and decide if this should be sent to SAP or not.\n Log on to the Azure Portal and look for Logic Apps  Create a new logic app with the following parameters:   Resource Group = IoTRG (created on the previous step) Logic App Name = SAPIOTLABXXXX (must be unique on Azure. Replace XXXX with a set of 4 numbers, like SAPIOTLAB1234) Region = East US Click on Review + create   Once the logic app is created, go to the Events on the left panel and select Logic Apps  The Logic Apps Designer will open with an Azure Event Grid setup. Confirm the Active Directoy to be used, click Sign In and log in with your credentials.  Once logged in, you will see a green checkmark. Click Continue  Let’s configure the Source event. configure the parameters as below and click on Next Step   Resource Type = Microsoft.Devices.IoTHubs Resource Name = Should be already populated, if not select your SAPIOTLAB IoT Hub Event Type = Microsoft.Devices.DeviceTelemetry   Search for Parse JSON and select Parse JSON as the next step  In Content, use dynamic content and select Body from “When a resource event occurs”. Click on Use payload to generate schema  Paste the sample payload below and click Done  {\t\t\"id\": \"\u003cTelemetry ID\u003e\",  \"topic\": \"/SUBSCRIPTIONS/\u003cSuscription ID\u003e/RESOURCEGROUPS/RG-AZUREIOT/PROVIDERS/MICROSOFT.DEVICES/IOTHUBS/ELBRUNOIOT\", \t\"subject\": \"devices/wioSquirrelFeeder\", \t\"eventType\": \"Microsoft.Devices.DeviceTelemetry\", \t\"data\": { \t\"properties\": {}, \t\"systemProperties\": { \t\"iothub-connection-device-id\": \"wioSquirrelFeeder\", \t\"iothub-connection-auth-method\": \"{\\\"scope\\\":\\\"device\\\",\\\"type\\\":\\\"sas\\\",\\\"issuer\\\":\\\"iothub\\\",\\\"acceptingIpFilterRule\\\":null}\", \t\"iothub-connection-auth-generation-id\": \"637769331872294921\", \t\"iothub-enqueuedtime\": \"2022-03-02T15:27:08.4210000Z\", \t\"iothub-message-source\": \"Telemetry\" \t}, \t\"body\": \"eyJhY3Rpb24iOiJhbmltYWwgZGV0ZWN0ZWQiLCJhbmltYWwiOiJzcXVpcnJlbCIsImZlZWRjb3VudCI6OCwiZmVlZGVyX3N0YXRlIjpmYWxzZSwicmFuZ2VfZGV0ZWN0ZWQiOjM3fQ==\" \t}, \t\"dataVersion\": \"\", \t\"metadataVersion\": \"1\", \t\"eventTime\": \"2022-03-02T15:27:08.421Z\" } 10. Add a new step after Parse JSON. 11. As you can see on the template above, the body for the document is base64 enconded. We need to convert it to string so we can handle the values sent by the device. Let’s add a Compose operation to convert this format: 12. On the Inputs field, we will use use dynamic content. Click on Expression and paste the expression below. Click OK decodeBase64(string(body('Parse_JSON')?['data']?['body'])) 13. Add another Parse JSON step, and rename it to Parse BODY. 14. On Parse Body, select content and add the Dynamic content Outputs from the previous Compose step. Click on Use payload to generate schema 15. Paste the sample payload below and click Done\n{  \"messageId\":5,  \"deviceId\":\"Raspberry Pi Web Client\",  \"temperature\":24.663457932335028,  \"humidity\":65.91259670696867 } Now we can read the data sent by the device. Let’s add the IF/THEN decision in the flow. Add a new step Condition (you can search for control)  Let’s define that temepratures above 29oC should trigger the IDOC to SAP to alert it. Add the field temperature from Parse Body…  … and as condition select is greater than and 29 . CLick on Add Action under the True result.  Inside the True result, add a new Compose step  Rename it to Build XML  As input, paste the XML Template below. This is a sample IDOC tempalte from SAP for flight data. Although it has nothing to do with your example, our focus is not about configuring the SAP side but instead focus on transmitting the message to SAP. This IDOC is already defined on SAP and ready to be used, which fits our objectives. We will replace the 3 XXX values with Dynamic Content as the picture below   AGENCYNUM = messageId PASSNAME = temperature PASSBIRTH = humidity Make sure you do not leave spaces before or after the dynamic values  \u003c?xml version=\"1.0\" encoding=\"UTF-8\" ?\u003e \u003cFLIGHTBOOKING_CREATEFROMDAT01\u003e \t\u003cIDOC BEGIN=\"1\"\u003e \t\u003cEDI_DC40 SEGMENT=\"1\"\u003e \t\u003cTABNAM\u003eEDI_DC40\u003c/TABNAM\u003e \t\u003cMANDT\u003e000\u003c/MANDT\u003e \t\u003cDOCREL\u003e750\u003c/DOCREL\u003e \t\u003cSTATUS\u003e56\u003c/STATUS\u003e \t\u003cDIRECT\u003e2\u003c/DIRECT\u003e \t\u003cOUTMOD/\u003e \t\u003cIDOCTYP\u003eFLIGHTBOOKING_CREATEFROMDAT01\u003c/IDOCTYP\u003e \t\u003cCIMTYP/\u003e \t\u003cMESTYP\u003eFLIGHTBOOKING_CREATEFROMDAT\u003c/MESTYP\u003e \t\u003cSNDPOR\u003eA000000002\u003c/SNDPOR\u003e \t\u003cSNDPRT\u003eLS\u003c/SNDPRT\u003e \t\u003cSNDPRN\u003eS4HCLNT100\u003c/SNDPRN\u003e \t\u003cRCVPOR\u003eA000000002\u003c/RCVPOR\u003e \t\u003cRCVPRT\u003eLS\u003c/RCVPRT\u003e \t\u003cRCVPRN\u003eS4HCLNT100\u003c/RCVPRN\u003e \t\u003cCREDAT\u003e20191001\u003c/CREDAT\u003e \t\u003cCRETIM\u003e090207\u003c/CRETIM\u003e \t\u003c/EDI_DC40\u003e \t\u003cE1SBO_CRE SEGMENT=\"1\"\u003e \t\u003cRESERVE_ONLY\u003eX\u003c/RESERVE_ONLY\u003e \t\u003cTEST_RUN\u003eX\u003c/TEST_RUN\u003e \t\u003cE1BPSBONEW SEGMENT=\"1\"\u003e \t\u003cAIRLINEID\u003eAA\u003c/AIRLINEID\u003e \t\u003cCONNECTID\u003e1234\u003c/CONNECTID\u003e \t\u003cFLIGHTDATE\u003e01012022\u003c/FLIGHTDATE\u003e \t\u003cCUSTOMERID\u003e00001234\u003c/CUSTOMERID\u003e \t\u003cCLASS\u003eB\u003c/CLASS\u003e \t\u003cCOUNTER\u003eALERT\u003c/COUNTER\u003e \t\u003cAGENCYNUM\u003eXXX\u003c/AGENCYNUM\u003e \t\u003cPASSNAME\u003eXXX\u003c/PASSNAME\u003e \t\u003cPASSBIRTH\u003eXXX\u003c/PASSBIRTH\u003e \t\u003c/E1BPSBONEW\u003e \t\u003cE1BPPAREX/\u003e \t\u003c/E1SBO_CRE\u003e \t\u003c/IDOC\u003e \u003c/FLIGHTBOOKING_CREATEFROMDAT01\u003e 22. Finally, still inside the True result and under Build XML, let’s add the last step so we can send data to SAP. Filter for message to SAP and select Send message to SAP 23. We will need to configure the connection to SAP. use the following parameters:\n Connection Name = S4HANA Data Gateway = Your subscrition and the gateway name you picked on the environment setup. Client = 100 Authentication Type = Basic SAP Username = BPINST SAP Password = Welcome1 AS Host = Your SAP HANA Public IP address as displayed in SAP CAL AS Service = 50000 AS System Number = 00 Leave the remaining parameters as the default   Under SAP Action we will define the IDOC template FLIGHTBOOKING_CREATEFROMDAT. In order to to this, click on the folder icon and on the arrow next to IDOC  Navigate on all the available IDOCs until FLIGHTBOOKING_CREATEFROMDAT is found and click on the arrow.  Again click on the arrow for the single item:  Scroll down to the last option. It should be version 755 or above. hover the mouse to the name and wait for the pop up to confirm. Click on the arrow.  Finally select SEND as the action itself.  In Input Message set Outputs as dynamic content from BUILD XML step  Click SAVE   Allright, we have created an LogicApp to handle the input from the IoT Device and send it to SAP in case temperaature is higher than 29oC.\nOn the next step we will test it and check on the SAP side for the message that can be later handled but ABAP code.\n",
    "description": "",
    "tags": null,
    "title": "Building the LogicApp",
    "uri": "/labs/iot/iot/step2/"
  },
  {
    "content": "In this step we will add some filtering functionality to the app and consume data from related Suppliers API based on the selected item.\nAdding a Category Filter   Click on the + icon, open Input, drag and drop the Drop down component to the canvas, resize it and set the required parameters:\n Name = DropProduct Items = Distinct(zProducts,Category)  The Items property is the content we want the component to have. In this example we will be selecting unique Category values from the zProduct collection. You can ALT+CLICK or OPTION+CLICK the button to test it.   Click on the + icon, open Input, drag and drop the Check box component to the canvas, name it Checkbox1 and set its Text to Enable filter   Now let’s change the Items property of ther Gallery1 to display all items or only display filtered Items, depending on the selections for the checkbox and the dropdown:\n Items = If(Checkbox1.Value, SortByColumns(Filter(zProducts, Category = DropProduct.Selected.Result), “Name”), SortByColumns(zProducts, “Name”))  Here we are running a little script with a conditional statement. If checkbox is checked, it will filter zProduct based on the dropdown selected. Otherwise, it will display all items.   We just added Filtering with almost no code at all. Click on the Play button to run the App and try the new components Adding More Data Now that the app is running with Products data, let’s add Suppliers Data to the app.\nThe leyout used on this example is a suggestion but feel free to rearrange the components and customize layout of your app.\n  We will start by loading more data on the Reload Products button. Change the OnSelect property to the value below. It will create a new collection called zSuppliers with data from the BusinessPartners API.\n OnSelect = ClearCollect(zProducts, ‘SAPODATA[Sample]Connector’.ListProductSets().d.results); ClearCollect(zSuppliers, ‘SAPODATA[Sample]Connector’.ListBusinessPartner().d.results)     In the same way we loaded more data on the step above, we will filter more data when selecting a product. Click on the arrow inside Gallery1 and change the OnSelect property to the value below. This will set a variable called zCurrentBP with the data from zSuppliers with the same SupplierID that the one in the product.\n OnSelect = Set(zSelected, Gallery1.Selected); Set (zCurrentBP, First(Filter(zSuppliers, BusinessPartnerID = Gallery1.Selected.SupplierID)))     Add more labels like we did on the app creation and set its Text property to:\n Supplier Email = zCurrentBP.EmailAddress Address = zCurrentBP.Address.Street\u0026\", “\u0026zCurrentBP.Address.Building\u0026”, “\u0026zCurrentBP.Address.PostalCode Country = zCurrentBP.Address.Country Phone = zCurrentBP.PhoneNumber Website = zCurrentBP.WebAddress     You can click on the play button on the top right to run the app. Reload the product and see it getting data from distinct APIs in a single screen. Let’s SAVE the App Click on the File menu: Congratulations, on the next section we will publish the app to Teams.\n",
    "description": "",
    "tags": null,
    "title": "Enhance the App",
    "uri": "/labs/autoint/powerapps/step3/"
  },
  {
    "content": "Chapter X Some Chapter title Lorem Ipsum.\n",
    "description": "",
    "tags": null,
    "title": "Office365",
    "uri": "/labs/dataai/office/"
  },
  {
    "content": "Chapter X Some Chapter title Lorem Ipsum.\n",
    "description": "",
    "tags": null,
    "title": "Sentinel",
    "uri": "/labs/security/sentinel/"
  },
  {
    "content": "Some demos will demand mock data and ODATA APIs to demosntrate SAP integration. Once the purpose of this demos are to show the integration itself, and not how to build those APIs, we will leverage an ready to be used SAP Gateway Demo System with everything we need.\nThis is a free service for which we need to create and account and define credentials for access. In this sectiopn we will go thru the steps required for you to have an account and credentials for it.\nWhat you will need  Existing SAP Account ID/password (same one used on SAP CAL or create a new one)  Expected duration This section is estimated to last no longer than 10 minutes\nAdditional Information and Links Creating an Account\nSAP Gateway Demo Login Screen\nSAP Gateway Demo updates and news\n",
    "description": "",
    "tags": null,
    "title": "SAP Gateway Demo System",
    "uri": "/env_setup/demogw/"
  },
  {
    "content": "In this step we will publish the app to Teams. Once the app is saved, it will be a few simple steps to integrate it to all users under your organization.\n  After the app being Saved, click on the Publish button   Another option is to click on the Apps menu, selecting the App, clicking the elippsis and selecting Add to Teams   It will ask for a confirmation, Click Add to Teams   Go to Teams and select the app on the left menu. Congratulations, you just created an App, consuming SAP Data from SAP Gateway, with simple mouse clicks and almost no lines of code !\n",
    "description": "",
    "tags": null,
    "title": "Publish to Teams",
    "uri": "/labs/autoint/powerapps/step4/"
  },
  {
    "content": "In this step we will test what we have built.\nThe steps required are:\n After saving the flow (last step on section #3), click on Test on the top right corner  Select Manually and click Test  Click Continue  Finally click on Run flow  You will be redirected to the results page. Status should be Your flow is running (this test takes around 15s).  You can see the status on each step, and clicking them you can see the details of inputs and outputs of each step.  After 15s-20s your flow should have run successfully  Let’s check the inserted data ! Go to the Fiori Launchpad and click on Manage Products  Filter by the 3 letter prefix you used on your Excel file (in this example PRD-) and click Go. You should see the inserted data.   Congratulations ! You just created an automation, with zero lines of code, to bulk create new products on SAP using existing ODATA interfaces by uploading a simple Excel file to OneDrive ! This lab can be customized to include different triggers, like a button on a Power App or an event, or an email with specific words on the subject…\n",
    "description": "",
    "tags": null,
    "title": "Testing the Automation",
    "uri": "/labs/autoint/automate/office365flow/step4/"
  },
  {
    "content": "Let’s build a Logic App to consume the data and decide if this should be sent to SAP or not.\n Go to the Raspberry Pi Azure IoT Online Simulator and click Run. Once you see a couple of messages with temperature above 29oC, click Stop  On the Logic Apps page, go to Overview and check the Runs history for the executions. Each message should have executed the flow once.  If you click on the run itself, it will present all the detais of inputs and outputs of every step in the flow. This is useful for debugging in case your run failed.  On the SAP Bastion Host, via Remote Desktop, open SAP GUI and go to TCODE WE02  Accept the defaults (new idocs created from midgnith until now) and click on the Run icon. Adjust the values if needed to math the current date.  Here you can see all the IDOCs received. Double click the last one.  Open Data Records, then open E1SBO_CRE and double click on the E1SBONEW. You will see the XML content as IDOC. Look at the AGENCYNUM, PASSNAME and PASSBIRTH values that were replaced by the same values sent by the device.   Congratulations! You just automated the data flow from an IoT device directly to SAP via Azure Logic Apps.\n",
    "description": "",
    "tags": null,
    "title": "Testing the lab",
    "uri": "/labs/iot/iot/step3/"
  },
  {
    "content": "This section will show how you can create and account in SAP Gateway Demo System and have access to the sample data models.\n  Go to the Signup page\n  Authenticate using your SAP credentials   Read and Accept the terms and click on Register   Wait for the email with user id (SXXXXXXXXXXX) and temporary password.   Log on using the temporary credentials to SAP Web GUI and change your password.   You should be able to see SAP GUI Web with your credentials. You can close the browser now. We will use the userid and password in the upcoming labs.   Congratulations ! You finished the setup of the environment for our labs to be created !\nNow select the desired lab on the left to continue.\n",
    "description": "",
    "tags": null,
    "title": "Creating and account",
    "uri": "/env_setup/demogw/step1/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/categories/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/tags/"
  }
]
