[
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/env_setup/azure_setup/",
	"title": "Azure Setup for SAP CAL",
	"tags": [],
	"description": "",
	"content": "In order to allow for SAP to deploy an S/4HANA environment in your Azure subscription, we need to setup permissions for it.\nWhat you will need  Azure subscription Admin access A text file to make note of the required information on the next step  Expected duration This section is estimated to last no longer than 10 minutes\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/",
	"title": "SAP on Microsoft",
	"tags": [],
	"description": "",
	"content": "SAP on Microsoft labs Welcome to the SAP on Microsoft Labs! This website is aimed to provide step-by-step guidance on how to integrate SAP systems with Microsoft services and software.\nWe know that for an environment like SAP, having a robust infrastrcuture is a must-have, but with Microsoft you can do much more!\nThis website is deivided in 3 main sections:\n Environment Setup SAP on Microsoft Labs Environment Cleanup  Both sections can be accesses on the left menu and each one will be divided into smaller chunks of steps.\nEach lab can be run individually from each other and are divided into sub-sections:\n Automation and Integration Data \u0026amp; AI Security Infrastructure and Management  On each lab introduction page, you will be presented by a demo video with the lab final result.\nSome components used will depend on Trial licenses that expire after a given period of time. Before building up on top of those labs, make sure you do this in a properly licensed enviroment to avoid losing your work.\n Requirements Before we start extending SAP, you need to have:\n An Azure subscription with enough credits An Office365 subscription with access to Power Platform  ATTENTION: Some VMs for SAP HANA used in this lab can become quite expensinve if you let it running. Make sure you define shutdown timers and in the end of your labs, go thru the Environment Cleanup section to remove unecessary resources\n Running the Labs It is strongly recommended that you go thru Environment Setup before anything else, once the Labs assume the standards deployed in the previous section.\nAlso for the labs themselves, follow the steps in the order they were designed to otherwise you risk having to startover from scratch.\nAll set Are you ready to get started?\nClick on the right arrow for next page or navigate using the left side menu.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/dataai/datafactory/step1/",
	"title": "Environment Preparation",
	"tags": [],
	"description": "",
	"content": "Before we start extracting data from SAP, we need to create a repository to hold the extracted data.\nIn this example we will be using Azure Data Factory to extract the data and store it in a Blob Storage of a Datalake.\nThis section will show the steps required to prepare the environment for that:\n Log on to the Azure Portal and look for Storage Accounts  Create a new Storage Account and provide the required information. Select the same region that SAP was deployed:   Resource Group: SAP_ADF (create new RG) Storage Account Name: sapadfXXXX (must be unique on Azure, change XXXX by any set of 4 numbers) Region: East US Redundancy: LRS   On the newly created storage account, select Containers on the left pane and click on + Container  Name it cntsapadf and accept the default security of Private, once we don\u0026rsquo;t want to expose business data to the internet.   With those 4 steps we have created a repository to store the data.\nLet\u0026rsquo;s create the Azure Data Factory instance now:\n Log on to the Azure Portal and look for Data factories  Create a new Data factory and provide the required information. Select the same region that SAP was deployed:   Resource Group: SAP_ADF Name: sapadfXXXX (change XXXX by any set of 4 numbers) Region: East US   Click Next or select the tab Git configuration and check Configure Git later once we will not be storing our pipelines in a repository in this example. After this, click Create   Now we will configure the Integration Runtime on our Bastion Host to have access to the SAP data:\n On the Data Factory overview page, click on Open Azure Data Factory Studio  On the new tab that will open, create a New Pipeline  Name it adfsap_pipeline on the right tab that will open:  Click on the Toolbox icon on the left pane, then click on Integration runtimes and click on New. Select the Self Hosted option  On the Integration runtime setup step, select Self Hosted again  And name it SAPIntegrationRuntime; click Create  Once we are not performing those action on the Bastion host itself, we will choose Option 2: Manual Setup.   Copy the Key 1 Authentication Key that will be used on the runtime installation to some scratch area. Right click and Copy the URL from the Step 1 (we will use it to download the software on the bastion host)   Go to the Bastion Host via RDP and download the latest version, using the URL provided in the step #7:  Install the downloaded software:  On the setup phase, paste the Authentication Key from step #7, and click Register:  For this example, we will Enable remote access from intranet and click Next:  The Integration Runtime will contact the Data FActory using the Authentication Key provided and register itself. Click on Launch Configuration Manager and confirm that everything was OK:  All set on the Bastion Host, we will go back to the Azure Portal. Let\u0026rsquo;s make sure the Integration runtime is showing up on the Data Factory. Click on Refresh button and it should show the newly registered SAPIntegrationRuntime:   Now, finally, we are ready to start extracting data from SAP!\nThe next 2 steps will be similar, divided by data provider.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/inframgmt/monitoring/step1/",
	"title": "Setting up Azure Monitor for SAP",
	"tags": [],
	"description": "",
	"content": "In order to have SAP monitoring, we need to spin up an instance of Azure Monitoring.\nThis section will show the steps required for that to be accomplished:\n Log on to the Azure Portal and look for Azure Monitor for SAP  Create a new instance and provide the required information. Select the same region, vNet and subnet as SAP was deployed:   Region: East US vNET: SAPCALDefault-eastus Subnet: default   Do not worry about defining providers at this moment. Click on + Container  Once the deployment is complete Go to Resource and let\u0026rsquo;s start configuring the data Providers   Next steps will be divided by data provider.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/dataai/datafactory/",
	"title": "Azure Data Factory",
	"tags": [],
	"description": "",
	"content": "Extracting SAP DATA with Azure Data Factory Estimated Time for this Lab This lab is estimated to take around 30 minutes.\nWhat we will build   "
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/inframgmt/monitoring/",
	"title": "Monitoring SAP",
	"tags": [],
	"description": "",
	"content": "Azure Monitoring for SAP One of the benefits of monitoring SAP on Azure is the integrated view of both Infrastructure metrics (like CPU, RAM and Disk) alongside HANA database metrics (like memory usage and memory growth), and Netweaver metrics like concurrent sessions and locks.\nThis allows for a rapid response in case of an incident response as well as provinding a simpler and more integrated way of having the whole SAP performance picture in a glance.\nThis lab is based on the Azure Monitor for SAP Quickstart guide.\nEstimated Time for this Lab This lab is estimated to take between 20 and 30 minutes.\nWhat we will build   "
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/autoint/virtualagents/step1/",
	"title": "Creating our Chatbot",
	"tags": [],
	"description": "",
	"content": "In this section we will create our Chatbot on Microsoft Power Platform.\nGo to Office 365, authenticate and select Power Apps. On Power Apps, expand Chatbots, select Create and click on the Basic conversational bot button. Fill the information to create the bot:\n Name: SAP Bot Language: Deisred language (examples in Brazilian Portuguese with translation) Environment: US   Once the bot is created, go to Topics and click on New Topic Let\u0026rsquo;s rename the newly created topic from Untitled to PO Details (en-US) / Detalhes do Pedido (pt-BR) The bot needs know which phrases trigger this topic, so we will add some examples of questions users may pose to accomplish what they need:\n en-US  I need details for a PO I want PO details I need to know line items of a purchase order Inform the products in a purchase order   pt-BR  Quero detalhes de uma ordem de compra Preciso detalhes de um pedido Quero saber os detalhes de uma PO Informar produtos em um pedido     Let\u0026rsquo;s add a new step, acknowledging the intent and asking the PO number to be queried, and another one asking the PO Number:\n Message - Acknowledge  en-US  Ok. If I understood it correctly you with to know the line items of a Purchase Order (PO). I will need some extra info for that.   pt-BR  Ok, entendi que você precisa de uma lista dos items de uma Ordem de Compra (PO). Para isso vou precisar do número do pedido.     Question  en-US  Can you inform the PO number? (exactly 10 chars - leading zeroes)   pt-BR  Você pdoeria informar o número do pedido? (exatamente 10 caracteres - zeros a esquerda)   Parameters:  Identify: User\u0026rsquo;s Entire Response Variable Name: PONumber Type: Text (SAP compares strings so that is why we have leading zeroes and exactly 10 chars) Usage: Bot (it allows us to use this info for further questions)       Again, let\u0026rsquo;s acknowledge the user input and add an action to query SAP.\n Message - Acknowledge  en-US  Thank you! Searching for order \u0026ldquo;xxx\u0026rdquo; details \u0026hellip;   pt-BR  Obrigado! Pesquisando a ordem \u0026ldquo;xxx\u0026quot;para você \u0026hellip;   IMPORTANT: replace xxx with bot.PONumber from dynamic values, like example below   Add Action:  Add a Call an Action and click on Create a flow     Now on the Flow we will:\n Rename the Flow to GetSAPOrderItems Define an Input variable called PONumber (type Text) inside the flow. Later on we will map this to the Bot.PONumber parameter. We will show a table on the bot\u0026rsquo;s answer. For this we will add an Initialize Variable, call it OutputTable (Type String) and add the following markdown content (don\u0026rsquo;t forget to add new line at the end) that will be rendered as a table.  | Date | Item | Description | Quantity | Price | |-----------|:-----------:|:-----------:|:-----------:|-----------:|  Next we will invoke SAP BAPI method:\n Add a Call SAP function step Set the required parameters:  AS Host: SAP HANA public IP Client: 100 AS System Number: 00 SAP Function Name: BAPI_SALESORDER_GETSTATUS SALESDOCUMENT: PONumber variable from dynamic values     Let\u0026rsquo;s now teach the Flow how to interpret SAP\u0026rsquo;s response:\n Add a Parse JSON action Generate Schema based on the JSON sample below by clicking on Generate from Sample and pasting it. Once the Schema is generated, define Content parameter as STATUSINFO from Dynamic Values  [  {  \u0026#34;DOC_NUMBER\u0026#34;: \u0026#34;0000000728\u0026#34;,  \u0026#34;DOC_DATE\u0026#34;: \u0026#34;2018-11-06\u0026#34;,  \u0026#34;PURCH_NO\u0026#34;: \u0026#34;xcwer\u0026#34;,  \u0026#34;PRC_STAT_H\u0026#34;: \u0026#34;C\u0026#34;,  \u0026#34;DLV_STAT_H\u0026#34;: \u0026#34;C\u0026#34;,  \u0026#34;REQ_DATE_H\u0026#34;: \u0026#34;2018-11-06\u0026#34;,  \u0026#34;DLV_BLOCK\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;ITM_NUMBER\u0026#34;: \u0026#34;000010\u0026#34;,  \u0026#34;MATERIAL\u0026#34;: \u0026#34;CM-FL-V01\u0026#34;,  \u0026#34;SHORT_TEXT\u0026#34;: \u0026#34;Forklift\u0026#34;,  \u0026#34;REQ_DATE\u0026#34;: \u0026#34;2018-11-21\u0026#34;,  \u0026#34;REQ_QTY\u0026#34;: \u0026#34;1.000\u0026#34;,  \u0026#34;CUM_CF_QTY\u0026#34;: \u0026#34;1.000\u0026#34;,  \u0026#34;SALES_UNIT\u0026#34;: \u0026#34;ST\u0026#34;,  \u0026#34;NET_VALUE\u0026#34;: \u0026#34;8000.00\u0026#34;,  \u0026#34;CURRENCY\u0026#34;: \u0026#34;USD\u0026#34;,  \u0026#34;NET_PRICE\u0026#34;: \u0026#34;8000.00\u0026#34;,  \u0026#34;COND_P_UNT\u0026#34;: \u0026#34;1\u0026#34;,  \u0026#34;COND_UNIT\u0026#34;: \u0026#34;ST\u0026#34;,  \u0026#34;DLV_STAT_I\u0026#34;: \u0026#34;C\u0026#34;,  \u0026#34;DELIV_NUMB\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;DELIV_ITEM\u0026#34;: \u0026#34;000000\u0026#34;,  \u0026#34;DELIV_DATE\u0026#34;: \u0026#34;0000-00-00\u0026#34;,  \u0026#34;DLV_QTY\u0026#34;: \u0026#34;0.000\u0026#34;,  \u0026#34;REF_QTY\u0026#34;: \u0026#34;0.000\u0026#34;,  \u0026#34;S_UNIT_ISO\u0026#34;: \u0026#34;PCE\u0026#34;,  \u0026#34;CD_UNT_ISO\u0026#34;: \u0026#34;PCE\u0026#34;,  \u0026#34;CURR_ISO\u0026#34;: \u0026#34;USD\u0026#34;,  \u0026#34;MATERIAL_EXTERNAL\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;MATERIAL_GUID\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;MATERIAL_VERSION\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;PO_ITM_NO\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;CREATION_DATE\u0026#34;: \u0026#34;0000-00-00\u0026#34;,  \u0026#34;CREATION_TIME\u0026#34;: \u0026#34;00:00:00\u0026#34;,  \u0026#34;S_UNIT_DLV\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;DLV_UNIT_ISO\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;REA_FOR_RE\u0026#34;: \u0026#34;70\u0026#34;,  \u0026#34;PURCH_NO_C\u0026#34;: \u0026#34;xcwer\u0026#34;,  \u0026#34;MATERIAL_LONG\u0026#34;: \u0026#34;CM-FL-V01\u0026#34;  },  {  \u0026#34;DOC_NUMBER\u0026#34;: \u0026#34;0000000728\u0026#34;,  \u0026#34;DOC_DATE\u0026#34;: \u0026#34;2018-11-06\u0026#34;,  \u0026#34;PURCH_NO\u0026#34;: \u0026#34;xcwer\u0026#34;,  \u0026#34;PRC_STAT_H\u0026#34;: \u0026#34;C\u0026#34;,  \u0026#34;DLV_STAT_H\u0026#34;: \u0026#34;C\u0026#34;,  \u0026#34;REQ_DATE_H\u0026#34;: \u0026#34;2018-11-06\u0026#34;,  \u0026#34;DLV_BLOCK\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;ITM_NUMBER\u0026#34;: \u0026#34;000020\u0026#34;,  \u0026#34;MATERIAL\u0026#34;: \u0026#34;CM-FL-V00\u0026#34;,  \u0026#34;SHORT_TEXT\u0026#34;: \u0026#34;Forklift\u0026#34;,  \u0026#34;REQ_DATE\u0026#34;: \u0026#34;2018-11-06\u0026#34;,  \u0026#34;REQ_QTY\u0026#34;: \u0026#34;7.000\u0026#34;,  \u0026#34;CUM_CF_QTY\u0026#34;: \u0026#34;0.000\u0026#34;,  \u0026#34;SALES_UNIT\u0026#34;: \u0026#34;ST\u0026#34;,  \u0026#34;NET_VALUE\u0026#34;: \u0026#34;58800.00\u0026#34;,  \u0026#34;CURRENCY\u0026#34;: \u0026#34;USD\u0026#34;,  \u0026#34;NET_PRICE\u0026#34;: \u0026#34;8400.00\u0026#34;,  \u0026#34;COND_P_UNT\u0026#34;: \u0026#34;1\u0026#34;,  \u0026#34;COND_UNIT\u0026#34;: \u0026#34;ST\u0026#34;,  \u0026#34;DLV_STAT_I\u0026#34;: \u0026#34;C\u0026#34;,  \u0026#34;DELIV_NUMB\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;DELIV_ITEM\u0026#34;: \u0026#34;000000\u0026#34;,  \u0026#34;DELIV_DATE\u0026#34;: \u0026#34;0000-00-00\u0026#34;,  \u0026#34;DLV_QTY\u0026#34;: \u0026#34;0.000\u0026#34;,  \u0026#34;REF_QTY\u0026#34;: \u0026#34;0.000\u0026#34;,  \u0026#34;S_UNIT_ISO\u0026#34;: \u0026#34;PCE\u0026#34;,  \u0026#34;CD_UNT_ISO\u0026#34;: \u0026#34;PCE\u0026#34;,  \u0026#34;CURR_ISO\u0026#34;: \u0026#34;USD\u0026#34;,  \u0026#34;MATERIAL_EXTERNAL\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;MATERIAL_GUID\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;MATERIAL_VERSION\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;PO_ITM_NO\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;CREATION_DATE\u0026#34;: \u0026#34;0000-00-00\u0026#34;,  \u0026#34;CREATION_TIME\u0026#34;: \u0026#34;00:00:00\u0026#34;,  \u0026#34;S_UNIT_DLV\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;DLV_UNIT_ISO\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;REA_FOR_RE\u0026#34;: \u0026#34;70\u0026#34;,  \u0026#34;PURCH_NO_C\u0026#34;: \u0026#34;xcwer\u0026#34;,  \u0026#34;MATERIAL_LONG\u0026#34;: \u0026#34;CM-FL-V00\u0026#34;  }  ] As you can see, the response is an array, so we have to use Apply to Each action, based on the Body dynamic output of Parse JSON Inside the For Each loop, we will be appending the lines one by one to the outputTable varibale which had only the header so far. For each item in the array it will add a new line. We will add an Append to string variable action and use \u0026ldquo;|\u0026rdquo; to separate the dynamic values.\nIMPORTANT: Remember to include opening and closing \u0026ldquo;|\u0026rdquo; and Add a NEW LINE in the end\n Dynamic values used to build the line are: DOC_DATE, MATERIAL, SHORT_TEXT, REQ_QTY, NET_PRICE   The last step on the Flow is to return the OutputTable variable back to the bot. Add a Return value to Power Virtual Agents action after the For Each (not inside it) and create a variable called outputTable (Type String) which will have the dynamic value OutputTable Save the Flow and let\u0026rsquo;s go back to the bot\u0026rsquo;s topic itself.\nNow we should be able to see the newly created Flow GetSAPOrderItens when adding the Call an action Here we will link the bot\u0026rsquo;s variables to the Flow ones. Map PONumber to bot.PONumber. It should autmatically get the outputTable variable from the flow. Now we will show the outputTable variable (string) as part of the message. Add the text and use the outputTable dynamic value in the content. Bot will render it as table on Teams later on.\n en-US  Here are the order details outputTable   pt-BR  Aqui estão os detalhes do pedido outputTable     Finally, add an End action and SAVE\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/autoint/logicapps/step1/",
	"title": "Create a Logic App",
	"tags": [],
	"description": "",
	"content": "In this step we will create a Logic App on Azure Portal.\n Go to Logic Apps Click on Add Fill the required parameters as the picture below  Subscription and Resource Group Name: SAPDemo Region: East US (same one used on the previous steps) Plan type: Consumption     "
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/autoint/logicapps/",
	"title": "Azure Logic Apps",
	"tags": [],
	"description": "",
	"content": "Azure Logic Apps Estimated Time for this Lab This lab is estimated to take between 30 and 60 minutes.\nWhat we will build In this lab we will build an API and receives a JSON payload with the Sales Order to be queries on SAP and sends the required information via Outlook using native SAP and Office 365 integrations.\nWhenever this API is used, it triggers a Logic App (built with no code) that will analyze the payload, run a BAPI on SAP, process the response, build an HTML table and send it by email alongside with the BAPI JSON.\nYou can see a sample of the lab below:\n  "
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/autoint/virtualagents/",
	"title": "Power Virtual Agent",
	"tags": [],
	"description": "",
	"content": "Power Virtual Agent Estimated Time for this Lab This lab is estimated to take between 30 and 60 minutes.\nWhat we will build In this lab we will build an chatbot that answers Sales Order details, using Power Virtual Agents and publishing the bot to Microsoft Teams.\nWhenever this bot is invoked, it triggers a a flow that will call an SAP BAPI based on user inputs process the response, build an HTML table and send it back to the chatbot.\nYou can see a sample of the lab below:\n  "
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/inframgmt/backup/",
	"title": "Azure Backup for HANA",
	"tags": [],
	"description": "",
	"content": "Azure Backup for HANA Include INTRO\nEstimated Time for this Lab This lab is estimated to take between 90 and 120 minutes, mostly due to backup and restores processes.\nWhat we will build   "
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/autoint/",
	"title": "Automation and Integration",
	"tags": [],
	"description": "",
	"content": "Chapter X Some Chapter title Lorem Ipsum.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/env_setup/datagwsetup/step1/",
	"title": "Remote Desktop Setup",
	"tags": [],
	"description": "",
	"content": "In this section we will install the required components for Azure and Microsoft be able to connect to your SAP environment.\nThis section is a summary of the guide Install data gateway and Connect to SAP systems.\nConnect to the Remote Desktop as on the previous step and using a browser on the Bastion Host, download 3 pieces of software (links might have changed, please check above guides for the most up-to-date links):\n .Net Framework latest version On-premises data gateway SAP Connector  Install the .Net Framework following the standard process.\nInstall the On-premises Data Gateway and configure it:\n Accept the default path and terms of use:  Once installation completes, we will register the gateway. Use the same email address of the Azure subscription. It will open a sign in window for you to complete authentication using your azure credentials.\n Select Register a new gateway:  Give it a name and define a recovery key:  Select the region. For this demos, East US is the prefered one:  Installation on the Data gateway on the Bastion Host side is complete and you should see a screen similar to the one below, with both PowerBI and PowerApps showing up as Ready:   Install the SAP .NET Connector and make sure you select Register WMI provider provider and install Assemblies to GAC Now, let\u0026rsquo;s move to the Azure part, creating the Gateway on the cloud to interface with the on-premises data gateway.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/env_setup/sap_cal/accsetup/",
	"title": "SAP CAL Account Setup",
	"tags": [],
	"description": "",
	"content": "First you need to create a free SAP Cloud Appliance Library (CAL) account.\nGo to https://cal.sap.com/ and click on Log On.\nOn the Login page, select Register and fill the required information.\nOnce you got your approval e-mail from SAP, proceed to the SAP CAL website and Log On with your credentials, then follow the steps below:\n Go to Accounts and click on Create Account  Give it a name, select Microsoft Azure and fill the data with the information gathered.  Click on Test Connection to make sure the parameters are valid.   Alright, now we have the proper access to deploy a S/4HANA in your subscription.\nSAP CAL allows for a 30 day Trial of the Solutions available, where SAP licenses are waived for 30 days; only the cloud provider hosting fees apply. You can setup auto-shutdown and auto-terminate later on to make sure we keep costs low.\nIn order to deploy a Trial S/4HANA in you subscription:\n  Go to Solutions, pick the latest SAP S/4HANA Fully-Activated Appliance and click on Create Instance.   Select the newly created Account   Read and Accept the License terms of the Trial   Go to Advanced Mode so we can understand the parameters used on the deploy   Select your account.   Give your instance a name, description and validate the networking settings for the VNet. SAP will create a Default SAP CAL Network in case you have a new subscription. Select Public Static IP address so we can have remote access without the need for a VPN.   We won\u0026rsquo;t be deploying Business Objects so on the next step, you can desselect this VM. Later on you can change the VM types on azure to cheaper, smaller or newer ones. Scroll down and check all the parameters, ports, disk sizes so you can familiarize with the solution.   Provide a password for SAP Admin access that will be customized during the deployment. There will be local pre-defined users and admin users with this password. For more details see the Getting Started Guide for S/4HANA on SAP CAL with the default users and passwords.   Here we can adjust when the SAP will be available:\n By Schedule - Scheduled start and shutdown Suspend on an Exact Date - Scheduled to be running for XX number of days and then shutdown. Manual - If you do not plan to use it everyday. When you activate you can setup a shutdown time in hours in case you forget.  On the right side you can see the costs changing depending on the selection   Click Create on the bottom of the page, review the data and go grab lunch (it should take 2-3 hours for the deployment to be complete).   You can monitor the progress of the deployment in the Instances tab. Wait for Activated status.   When the solution is deployed and activated, click on the instance name and check the top menu, where you can control the SAP landscape (suspend/terminate/activate) as well as the remote desktop IP address used as Bastion Host for managing the environment on Azure. You can also access the Getting Started Guide under Solution Info tab as well make sure all the VMs are up and SAP was started properly and is communicating. All done ! You have an SAP S/4HANA running in your susbcription ! On the next section let\u0026rsquo;s see how we can access it and key users/passwords/parameters for conection that will be required later on.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/env_setup/sap_cal/",
	"title": "SAP Cloud Appliance Library (CAL)",
	"tags": [],
	"description": "",
	"content": "The SAP Cloud Appliance Library (CAL) offers a quick and easy way to consume the latest SAP solutions in the cloud, such as SAP S/4HANA, SAP HANA Express Edition, Industry Solutions etc.\nIt\u0026rsquo;s an online library of latest, preconfigured, ready-to-use SAP solutions that can be instantly deployed into your own public cloud accounts.\nIn this section we will create and account on SAP CAL, configure Azure permissions and deploy an S/4HANA environment on your subscription.\nWhat you will need  Parameters from previous step  Expected duration This section is estimated to last around 2-3 hours, mostly depending on the SAP deployment itself.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/env_setup/azure_setup/azuread/",
	"title": "Create and Azure Active Directory application",
	"tags": [],
	"description": "",
	"content": " Sign in to your Azure Account through the Azure Portal Select Azure Active Directory Select App Registrations  Select New Registration  Provide a name, accept defaults and select Register   Now let\u0026rsquo;s add a role to this application.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/env_setup/",
	"title": "Environment Setup",
	"tags": [],
	"description": "",
	"content": "Before we start building our labs we need to have provisioned a few resources:\n An Azure subscription setup An S/4HANA environment Installation of gateway components  The following sections will guide you thru the steps to accomplish this and be ready to the labs.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/dataai/datafactory/step2/",
	"title": "HANA Provider",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s create a new pipeline and extract data to the Blob Storage:\n On the Data Factory Studio, click on the Pencil icon on the left, click the Dataset Ellipsis and select New Dataset  For the datastore, we will search for SAP and select SAP HANA:  For the properties:   Name: SapHanaTable1 Create a New under Linked Service   Fill the required data for the linked service:   Name: SapHana1 Integration Runtime: SAPIntegrationRuntime Server Name: \u0026laquo;HANA IP\u0026raquo;:30215 Authentication Type: Basic User name: SAPHANADB Password: Password defined on the CAL setup phase Click on Test connection and if successful, click on Apply   Select the newly created SapHana1 linked service and click OK (we will not be selecting a table right now)   We are now communicating with SAP HANA. Let\u0026rsquo;s see some data from Materials:\n On the left pane, select the new Dataset created SapHanaTable1 and for Table select SAPHANADB.MATDOC. Click Preview Data. (it may take a while for it to discover all the existing tables)  You should see a sample of the table data. Take a look and close the box on the top right corner.   Alright, so by now we are communciating with SAP and have access to data. Let\u0026rsquo;s create the pipeline to extract it to Blob.\nJump to STEP 4 on this LAB: Extracting data (Step 3 will be the same we just did but accessing SAP by the Netweaver layer)\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/inframgmt/backup/step1/",
	"title": "Setup Azure Backup",
	"tags": [],
	"description": "",
	"content": "In order to have Azure Backup backing up SAP HANA databases, we need to setup the Azure Backup as well as the HANA Database Server.\nThis section will show the steps required for that to be accomplished:\nSAP HANA Setup We need to connect to the SAP HANA Server and run a script that will prepare the database for Azure Backup.\n Log on to the Azure Portal and open a Cloud Shell (BASH)  We need to SSH to the server, using the SAP provided certificate key (can be downloaded from SAP CAL again), previously uploaded to the CloudShell and having permission as 400).  mod@Azure:~$ ssh -i \u0026lt;KEYNAME\u0026gt;.pem root@\u0026lt;HANA PUBLIC IP\u0026gt; sid-hdb-s4h:~ #  Then we will change the user to the HANA administrator hdbadm, store the database password in a secure storage, and return to root.  sid-hdb-s4h:~ # sudo su - hdbadm sid-hdb-s4h:HDB:hdbadm /usr/sap/HDB/HDB02 2\u0026gt; hdbuserstore set azure_key localhost:30213 SYSTEM \u0026#39;\u0026lt;YOUR SAP DEFINED PASSWORD\u0026gt;\u0026#39; sid-hdb-s4h:HDB:hdbadm /usr/sap/HDB/HDB02 4\u0026gt; exit logout sid-hdb-s4h:~ #  As root, we will download and run the setup script:  sid-hdb-s4h:~ # wget https://go.microsoft.com/fwlink/?linkid=2173610 -O pre-script.sh --2022-03-14 19:44:01-- https://go.microsoft.com/fwlink/?linkid=2173610 Resolving go.microsoft.com (go.microsoft.com)... 184.50.50.164, 2600:1408:c400:e82::2c1a, 2600:1408:c400:e80::2c1a Connecting to go.microsoft.com (go.microsoft.com)|184.50.50.164|:443... connected. HTTP request sent, awaiting response... 302 Moved Temporarily Location: https://download.microsoft.com/download/B/2/E/B2E01EF8-C247-42A6-BCC7-E45B78F20C99/msawb-plugin-config-com-sap-hana.sh [following] --2022-03-14 19:44:01-- https://download.microsoft.com/download/B/2/E/B2E01EF8-C247-42A6-BCC7-E45B78F20C99/msawb-plugin-config-com-sap-hana.sh Resolving download.microsoft.com (download.microsoft.com)... 204.79.197.219 Connecting to download.microsoft.com (download.microsoft.com)|204.79.197.219|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 87552 (86K) [application/octet-stream] Saving to: ‘pre-script.sh’  sid-hdb-s4h:~ # chmod +x pre-script.sh sid-hdb-s4h:~ # ./pre-script.sh --system-key azure_key  [2022-03-14T19:45:08+00:00] [INFO] Checking if \u0026#39;root\u0026#39;. [2022-03-14T19:45:08+00:00] [PASS] Running as \u0026#39;root\u0026#39;. [2022-03-14T19:45:08+00:00] [INFO] Checking OS support. [2022-03-14T19:45:08+00:00] [PASS] Found supported OS_NAME_VERSION = \u0026#39;SLES-15.1\u0026#39;. [2022-03-14T19:45:08+00:00] [INFO] Checking for free space in \u0026#39;/opt\u0026#39;. [2022-03-14T19:45:08+00:00] [PASS] Found at least 2 GiB space on \u0026#39;/opt\u0026#39;. [2022-03-14T19:45:08+00:00] [INFO] Checking HOSTNAMES. [2022-03-14T19:45:08+00:00] [PASS] Found HOSTNAMES = [ [2022-03-14T19:45:08+00:00] [INFO] \u0026#39;::1\u0026#39; [2022-03-14T19:45:08+00:00] [INFO] \u0026#39;10.0.0.166\u0026#39; [2022-03-14T19:45:08+00:00] [INFO] \u0026#39;127.0.0.1\u0026#39; [2022-03-14T19:45:08+00:00] [INFO] \u0026#39;fe80::222:48ff:fe2e:bde7\u0026#39; ... [2022-03-15T17:56:40+00:00] [INFO] Checking login for BACKUP_KEY_USER = \u0026#39;AZUREWLBACKUPHANAUSER\u0026#39;. [2022-03-15T17:56:40+00:00] [PASS] Checked login. [2022-03-15T17:56:40+00:00] [INFO] Granting privilege \u0026#39;DATABASE ADMIN\u0026#39; to \u0026#39;AZUREWLBACKUPHANAUSER\u0026#39;. [2022-03-15T17:56:40+00:00] [PASS] Granted privilege. [2022-03-15T17:56:40+00:00] [INFO] Granting privilege \u0026#39;CATALOG READ\u0026#39; to \u0026#39;AZUREWLBACKUPHANAUSER\u0026#39;. [2022-03-15T17:56:40+00:00] [PASS] Granted privilege. [2022-03-15T17:56:40+00:00] [INFO] Granting privilege \u0026#39;INIFILE ADMIN\u0026#39; to \u0026#39;AZUREWLBACKUPHANAUSER\u0026#39;. [2022-03-15T17:56:40+00:00] [PASS] Granted privilege. [2022-03-15T17:56:40+00:00] [INFO] Granting privilege \u0026#39;BACKUP ADMIN\u0026#39; to \u0026#39;AZUREWLBACKUPHANAUSER\u0026#39;. [2022-03-15T17:56:40+00:00] [PASS] Granted privilege. [2022-03-15T17:56:40+00:00] [INFO] Checking privilege \u0026#39;CATALOG READ\u0026#39; on \u0026#39;AZUREWLBACKUPHANAUSER\u0026#39;. [2022-03-15T17:56:40+00:00] [PASS] Checked privilege. [2022-03-15T17:56:40+00:00] [INFO] Checking privilege \u0026#39;BACKUP ADMIN\u0026#39; on \u0026#39;AZUREWLBACKUPHANAUSER\u0026#39;. [2022-03-15T17:56:41+00:00] [PASS] Checked privilege. [2022-03-15T17:56:41+00:00] [INFO] Checking privilege \u0026#39;INIFILE ADMIN\u0026#39; on \u0026#39;AZUREWLBACKUPHANAUSER\u0026#39;. [2022-03-15T17:56:41+00:00] [PASS] Checked privilege. [2022-03-15T17:56:41+00:00] [INFO] Checking privilege \u0026#39;DATABASE ADMIN\u0026#39; on \u0026#39;AZUREWLBACKUPHANAUSER\u0026#39;. [2022-03-15T17:56:41+00:00] [PASS] Checked privilege. [2022-03-15T17:56:41+00:00] [INFO] Adding user \u0026#39;hdbadm\u0026#39; to group \u0026#39;msawb\u0026#39;. [2022-03-15T17:56:41+00:00] [PASS] Successfully added user. [2022-03-15T17:56:41+00:00] [INFO] Writing to configuration. [2022-03-15T17:56:41+00:00] [PASS] Writing complete. [2022-03-15T17:56:41+00:00] [INFO] Writing to environment file. [2022-03-15T17:56:41+00:00] [PASS] Writing complete. [2022-03-15T17:56:41+00:00] [SUCC] Done. At this point our HANA database is ready to be accessed by Azure backup and Backint (SAP Native Backup Agent) has been configured to send data to Azure.\nAzure Backup Setup  Log on to the Azure Portal and open Backup Center  On Backup Center we will create a Vault to store the backup data  For Vault Type select Recovery Services Vault  Provide the Vault details:   Region: East US (Same as HANA database deployment)   Once the Vault is created, let\u0026rsquo;s configure the Backup targets inside the Vault.  Change the backup type to SAP HANA on Azure VM and click on Start Discovery.  Select the HANA VM \u0026laquo;instance name-SAP1 and click on Discover DB at the bottom of the page  Azure will start a deployment for the agent. Wait until you see the sucessfull message on Notifications  Back to the Backup page, you should now have a View Details button. Click it so we can see the discovered Databases.  Here you can see the discovered databases. Check the info and then close then window to go back to Backup  Click on Configure Backup  You can accept the default policy, select an existing one or Edit the current policy.  In this lab we will create a simpler policy, called DemoPolicy. Click on Edit besides each item and configure it so it matches the example below.  Now let\u0026rsquo;s add the discovered HANA databases. Click in Add and on the window that will open, select the DBs.  Click on Enable Backup once you have selected the HDB and SYSTEMDB databases.  Now, let\u0026rsquo;s make sure the items were correctly added and prepare for the first backup. Click on Backup items and then SAP HANA in Azure VM  You should see a screen similar to this, with a Warning (initial backup pending) message   Congratulations, you have configured the HANA Backup using Azure Backup. Now let\u0026rsquo;s move to the next section and run our first manual backup.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/inframgmt/monitoring/step2/",
	"title": "HANA Provider",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s add a HANA Provider to collect metrics for us:\n On the Azure Monitor for SAP, go to Providers  Click Add  Let\u0026rsquo;s fill the required data:   Type: SAP HANA IP Address: SAP HANA Private IP Address Database tenant: SYSTEMDB SQL Port: 30215 Username: SAPHANADB Password: Password defined during deploy of SAP CAL   Wait for the Provider to be created and the data validated.  It takes about 10-15 minutes for information to be initially available on Azure Monitor. Once this time has passed, go to Monitoring -\u0026gt; SAP HANA on the left blade.  Select the desired HANA instance (you can monitor several instances like DEV/QAS/PRD on so on\u0026hellip;)   An from here on, you should be able to see collected data directly from SAP HANA:\nOverview with peak CPU and RAM, HANA services status and licenses) Historic utilization data for CPU and Memory Data size and growth And more data like Backups and System Checks for Save Points and Delta Merges Next step we will add operating system counter to the Azure Monitor for SAP.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/inframgmt/monitoring/step3/",
	"title": "Linux Provider",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s add a HANA Provider to collect metrics for us:\nNode Exporter Agent Before we start, we need to add an agent to the monitored VM.\n Go to the Azure Portal and open a Cloud Shell. In the examples we used BASH as the interpreter.  Now we will upload the key that SAP CAL generated on the deployment so we can log on to the Linux OS. Click on the File Transfer icon and then Upload  Select the key which will be a *.pem file, downladed from SAP CAL. If you do not remember or cannot find the file, go to SAP CAL, select your instance, click on Download Key and download it again.  Let\u0026rsquo;s add permissions to the .PEM file uploaded on the Azure Cloud Shell (in this example the key is called demosap.pem)  chmod 400 demosap.pem With the key setup done, we can connect to the SAP HANA instance, usign the key as password for root:  ssh -i demosap.pem root@\u0026lt;PUBLIC IP of HANA\u0026gt; Let\u0026rsquo;s download the agent. Go to Prometheus Download Page and copy the address for the latest Node Exporter for Linux  Back to Azure Cloud Shell, we will download, extract the agent and copy it to /usr/bin.  sid-hdb-s4h:~ # wget https://github.com/prometheus/node_exporter/releases/download/v1.3.1/node_exporter-1.3.1.linux-amd64.tar.gz sid-hdb-s4h:~ # tar xvzf node_exporter-1.3.1.linux-amd64.tar.gz  node_exporter-1.3.1.linux-amd64/ node_exporter-1.3.1.linux-amd64/LICENSE node_exporter-1.3.1.linux-amd64/NOTICE node_exporter-1.3.1.linux-amd64/node_exporter sid-hdb-s4h:~ # cd node_exporter-1.3.1.linux-amd64/ sid-hdb-s4h:~/node_exporter-1.3.1.linux-amd64 # cp -pr node_exporter /usr/bin/ Let\u0026rsquo;s test the agent by invoking it:  sid-hdb-s4h:~ # node_exporter ts=2022-03-14T18:58:29.764Z caller=node_exporter.go:182 level=info msg=\u0026#34;Starting node_exporter\u0026#34; version=\u0026#34;(version=1.3.1, branch=HEAD, revision=a2321e7b940ddcff26873612bccdf7cd4c42b6b6)\u0026#34; ts=2022-03-14T18:58:29.764Z caller=node_exporter.go:183 level=info msg=\u0026#34;Build context\u0026#34; build_context=\u0026#34;(go=go1.17.3, user=root@243aafa5525c, date=20211205-11:09:49)\u0026#34; ts=2022-03-14T18:58:29.764Z caller=node_exporter.go:185 level=warn msg=\u0026#34;Node Exporter is running as root user. This exporter is designed to run as unpriviledged user, root is not required.\u0026#34; ts=2022-03-14T18:58:29.765Z caller=filesystem_common.go:111 level=info collector=filesystem msg=\u0026#34;Parsed flag --collector.filesystem.mount-points-exclude\u0026#34; flag=^/(dev|proc|run/credentials/.+|sys|var/lib/docker/.+)($|/) ts=2022-03-14T18:58:29.765Z caller=filesystem_common.go:113 level=info collector=filesystem msg=\u0026#34;Parsed flag --collector.filesystem.fs-types-exclude\u0026#34; flag=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$ ts=2022-03-14T18:58:29.765Z caller=node_exporter.go:108 level=info msg=\u0026#34;Enabled collectors\u0026#34; ts=2022-03-14T18:58:29.765Z caller=node_exporter.go:115 level=info collector=arp ts=2022-03-14T18:58:29.765Z caller=node_exporter.go:115 level=info collector=bcache On the Bastion Host, go to http://\u0026laquo;SAP HANA Private IP\u0026raquo;:9100/metrics  Kill the node_exporter process with CTRL+C and start it in background  nohup node_exporter \u0026amp; The agent is installed and communicating internally on the vNET. Let\u0026rsquo;s add the provider now\nLinux Provider  On the Azure Monitor for SAP, go to Providers  Click Add  Let\u0026rsquo;s fill the required data:   Type: OS Linux Endpoint: http://internal_ip_address:9100/metrics   Wait until the provider is created and give it 10-15 minutes for data to start to flow.  Go to Monitoring, select OS (Linux), and pick the host.   Congratulations, you just installed and finished the OS Linux Agent install.\nFrom here on, you should be able to see collected data directly from Linux:\nOverview with peak CPU and RAM, disk usage Historic CPU Monitoring Disk size and growth "
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/autoint/virtualagents/step2/",
	"title": "Testing our Chatbot",
	"tags": [],
	"description": "",
	"content": "In this section we will test and debug our Chatbot on Microsoft Power Platform.\nOn the left side of the screen go to the Test bot and simulate a user conversation.\n IMPORTANT:  SAP expects a 10 char stirng with leading zeroes, so if you want to check order 728 you need to type 0000000728 Once we are doing a lab, we are not treating the input so type just the PO number with no other information, leading/trailing spaces, words, once we are passing the whole input to SAP. In a production bot more data treatment should be done, by creating a regexp and defining an Entity.     We should see the String we created on the Flow. Don\u0026rsquo;t worry with the format now, on Teams it will render as a table. If you need to debug the Flow, click on View flow details It will show all the flow runs and clicking on a run, will give you details step-by-step as well as more information for debug. Sucessful Run Example: Failed Run Example:\nIn this example I was logged on SAP GUI on exactly the required item, so I accidentally generated a lock that prevented the bot to query the data. Closing SAP GUI solved it. "
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/autoint/logicapps/step2/",
	"title": "Create a Workflow",
	"tags": [],
	"description": "",
	"content": "In this step we will create the Workflow on Azure Portal.\n Go to Logic Apps Click on your Logic App SAPDemo Go to Workflows and Add a new one as the parameters below:  Click on Designer and let\u0026rsquo;s start building the Workflow! As the first operation, which starts the flow, we will pick When a HTTP request is received  In order to customize the payload we expect, let\u0026rsquo;s Use a sample payload to generate schema  Sample payload: {\u0026quot;id\u0026quot;: \u0026quot;0000000728\u0026quot;}    This will generate a schema for the trigger  Now we will create our temp variable to hold the desired part of the SAP reponse. Add a new action and type \u0026ldquo;initialize\u0026rdquo; on the search box. Select Initialize Variables  In this step, we will create na empty array to store the data:  Name: outputArray Type: Array Value: []    Next we will invoke BAPI method in SAP by adding a new Action (make sure you select Azure ) called [BAPI] Call method in SAP  Now we need to link this BAPI call with the On-premises Data Gateway we installed on Bastion Host. Click on Change connection  An now we provide the connection information required so Logic Apps can communicate with SAP:  Connection Name: SAPS4CAL DataGateway: Select your Gateway, in this case SapDemoGW Client: 100 Auth: Basic User: BPINST Password: Welcome1 AS HOST: Public IP of your SAP HANA on SAP CAL (this IP will change if you shutdown SAP. You will have to follow the same procedure to manually update upon activation or resort to dynamic DNS) AS SERVICE: 50000 AS SYSTEM NUMBER: 00 Accept other defaults and click Create    Logic App will test the connection and you should see the Connected status  Now let\u0026rsquo;s setup the BAPI method to be invoked  Business Object: BUS2032:SalesOrder Method: GETSTATUS:Display Sales Order:BAPI_SALESORDER_GETSTATUS Input: \u0026lt;GETSTATUS xmlns=\u0026quot;http://Microsoft.LobServices.Sap/2007/03/Rfc/\u0026quot;\u0026gt;\u0026lt;SALESDOCUMENT\u0026gt;xxx\u0026lt;/SALESDOCUMENT\u0026gt;\u0026lt;/GETSTATUS\u0026gt; Replace xxx on the above Input with a Dynamic Content id extracted from the trigger on step 7. Make sure there are no spaces between SALESDOCUMENT    Add a new Parse JSON action  In content use JsonResponse that will be passed by BAPI  As we did on the initial step, select Use sample payload to generate schema and use the following JSON sample {\u0026quot;STATUSINFO\u0026quot;:[{\u0026quot;DOC_NUMBER\u0026quot;:\u0026quot;0000000728\u0026quot;,\u0026quot;DOC_DATE\u0026quot;:\u0026quot;2018-11-06\u0026quot;,\u0026quot;PURCH_NO\u0026quot;:\u0026quot;xcwer\u0026quot;,\u0026quot;PRC_STAT_H\u0026quot;:\u0026quot;C\u0026quot;,\u0026quot;DLV_STAT_H\u0026quot;:\u0026quot;C\u0026quot;,\u0026quot;REQ_DATE_H\u0026quot;:\u0026quot;2018-11-06\u0026quot;,\u0026quot;DLV_BLOCK\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;ITM_NUMBER\u0026quot;:\u0026quot;000010\u0026quot;,\u0026quot;MATERIAL\u0026quot;:\u0026quot;CM-FL-V01\u0026quot;,\u0026quot;SHORT_TEXT\u0026quot;:\u0026quot;Forklift\u0026quot;,\u0026quot;REQ_DATE\u0026quot;:\u0026quot;2018-11-21\u0026quot;,\u0026quot;REQ_QTY\u0026quot;:1.0,\u0026quot;CUM_CF_QTY\u0026quot;:1.0,\u0026quot;SALES_UNIT\u0026quot;:\u0026quot;ST\u0026quot;,\u0026quot;NET_VALUE\u0026quot;:8000.0,\u0026quot;CURRENCY\u0026quot;:\u0026quot;USD\u0026quot;,\u0026quot;NET_PRICE\u0026quot;:8000.0,\u0026quot;COND_P_UNT\u0026quot;:1.0,\u0026quot;COND_UNIT\u0026quot;:\u0026quot;ST\u0026quot;,\u0026quot;DLV_STAT_I\u0026quot;:\u0026quot;C\u0026quot;,\u0026quot;DELIV_NUMB\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;DELIV_ITEM\u0026quot;:\u0026quot;000000\u0026quot;,\u0026quot;DELIV_DATE\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;DLV_QTY\u0026quot;:0.0,\u0026quot;REF_QTY\u0026quot;:0.0,\u0026quot;S_UNIT_ISO\u0026quot;:\u0026quot;PCE\u0026quot;,\u0026quot;CD_UNT_ISO\u0026quot;:\u0026quot;PCE\u0026quot;,\u0026quot;CURR_ISO\u0026quot;:\u0026quot;USD\u0026quot;,\u0026quot;MATERIAL_EXTERNAL\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;MATERIAL_GUID\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;MATERIAL_VERSION\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;PO_ITM_NO\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;CREATION_DATE\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;CREATION_TIME\u0026quot;:\u0026quot;00:00:00\u0026quot;,\u0026quot;S_UNIT_DLV\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;DLV_UNIT_ISO\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;REA_FOR_RE\u0026quot;:\u0026quot;70\u0026quot;,\u0026quot;PURCH_NO_C\u0026quot;:\u0026quot;xcwer\u0026quot;,\u0026quot;MATERIAL_LONG\u0026quot;:\u0026quot;CM-FL-V01\u0026quot;},{\u0026quot;DOC_NUMBER\u0026quot;:\u0026quot;0000000728\u0026quot;,\u0026quot;DOC_DATE\u0026quot;:\u0026quot;2018-11-06\u0026quot;,\u0026quot;PURCH_NO\u0026quot;:\u0026quot;xcwer\u0026quot;,\u0026quot;PRC_STAT_H\u0026quot;:\u0026quot;C\u0026quot;,\u0026quot;DLV_STAT_H\u0026quot;:\u0026quot;C\u0026quot;,\u0026quot;REQ_DATE_H\u0026quot;:\u0026quot;2018-11-06\u0026quot;,\u0026quot;DLV_BLOCK\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;ITM_NUMBER\u0026quot;:\u0026quot;000020\u0026quot;,\u0026quot;MATERIAL\u0026quot;:\u0026quot;CM-FL-V00\u0026quot;,\u0026quot;SHORT_TEXT\u0026quot;:\u0026quot;Forklift\u0026quot;,\u0026quot;REQ_DATE\u0026quot;:\u0026quot;2018-11-06\u0026quot;,\u0026quot;REQ_QTY\u0026quot;:7.0,\u0026quot;CUM_CF_QTY\u0026quot;:0.0,\u0026quot;SALES_UNIT\u0026quot;:\u0026quot;ST\u0026quot;,\u0026quot;NET_VALUE\u0026quot;:58800.0,\u0026quot;CURRENCY\u0026quot;:\u0026quot;USD\u0026quot;,\u0026quot;NET_PRICE\u0026quot;:8400.0,\u0026quot;COND_P_UNT\u0026quot;:1.0,\u0026quot;COND_UNIT\u0026quot;:\u0026quot;ST\u0026quot;,\u0026quot;DLV_STAT_I\u0026quot;:\u0026quot;C\u0026quot;,\u0026quot;DELIV_NUMB\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;DELIV_ITEM\u0026quot;:\u0026quot;000000\u0026quot;,\u0026quot;DELIV_DATE\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;DLV_QTY\u0026quot;:0.0,\u0026quot;REF_QTY\u0026quot;:0.0,\u0026quot;S_UNIT_ISO\u0026quot;:\u0026quot;PCE\u0026quot;,\u0026quot;CD_UNT_ISO\u0026quot;:\u0026quot;PCE\u0026quot;,\u0026quot;CURR_ISO\u0026quot;:\u0026quot;USD\u0026quot;,\u0026quot;MATERIAL_EXTERNAL\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;MATERIAL_GUID\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;MATERIAL_VERSION\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;PO_ITM_NO\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;CREATION_DATE\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;CREATION_TIME\u0026quot;:\u0026quot;00:00:00\u0026quot;,\u0026quot;S_UNIT_DLV\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;DLV_UNIT_ISO\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;REA_FOR_RE\u0026quot;:\u0026quot;70\u0026quot;,\u0026quot;PURCH_NO_C\u0026quot;:\u0026quot;xcwer\u0026quot;,\u0026quot;MATERIAL_LONG\u0026quot;:\u0026quot;CM-FL-V00\u0026quot;}]}  SAP Sales Order can have multiple Line Items, so we have to iterate in them. Add na For Each action and as parameter, STATUSINFO from Parse JSON previous action.  Inside the For Each loop add a Compose action. It will allow for us to pick desired fields for each line and build a JSON that suits our needs.  Build the desired JSON using the sample JSON below and replacing fields enclosed by \u0026lt;\u0026gt; using Dynamic Inputs as inputs (make sure you keep the commas) { \u0026quot;Order\u0026quot;: \u0026lt;DOC_NUMBER\u0026gt;, \u0026quot;Date\u0026quot;: \u0026lt;DOC_DATE\u0026gt;, \u0026quot;Item\u0026quot;: \u0026lt;MATERIAL\u0026gt;, \u0026quot;Description\u0026quot;: \u0026lt;SHORT_TEXT\u0026gt;, \u0026quot;Quantity\u0026quot;: \u0026lt;REQ_QTY\u0026gt;, \u0026quot;Price\u0026quot;: \u0026lt;NET_PRICE\u0026gt; }   Inside the For Each loop, after Compose, add na action Append to array variable. Select outputArray previously initialized as empty and for Value Outputs from Compose action  Once For Each runs and populates outputArray with the desired data, we will convert it to an HTML table. Add a Create HTML table Action and use the outputArray as the data for the table.  Finally, we will add an action Send an email from Office 365 Outlook. You may need to connect Logic Apps with Office 365, by clicking on Change connection as did previously for SAP.  Let\u0026rsquo;s now build the email template that will be used on every invocation  To: Destination email Subject: Details for order  (replace with dynamic content) Body: as picture below    As the API response we will add a Response action and return  Status Code: 200 Body: JsonResponse from Dynamic inputs    Don\u0026rsquo;t forget to click SAVE !!!! ;)   If everything wen\u0026rsquo;t fine, we are ready to test the lab we just built, with NO CODE at all !\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/dataai/",
	"title": "Data &amp; AI",
	"tags": [],
	"description": "",
	"content": "Chapter X Some Chapter title Lorem Ipsum.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/env_setup/datagwsetup/step2/",
	"title": "Azure Data Gateway Setup",
	"tags": [],
	"description": "",
	"content": "In this section we will create the endpoint on Azure for the installed on-premises data gateway.\nThis section is a summary of the guide Install data gateway and Connect to SAP systems.\nConnect to the Azure Portal and follow the steps below:\n Go to On-premises Data Gateways:  Select Add  Fill the required parameters. Make sure to Select the same region as used on the installation (East US) and Select the name of the gateway previously created that should be populated on the drop-down box.   Congratulations, you just finished the setup of the environment for our labs to be created !\nNow select the desired lab on the left to continue.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/env_setup/sap_cal/rdpacc/",
	"title": "Remote Desktop Access",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s connect to an RDP session on the Bastion Host.\nUse the Public IP for the Remote Desktop provided on the Instance tab.\n Default User: Administrator Default Password: The one used during the SAP deployment  Once connected you should see a screen similar to this. If the message says it is still under deployment, log off and wait a little bit longer. We will basically use 3 apps installed here:\n Web browser SAP Logon (GUI) HANA Studio  The connection information for the SAP GUI is:\n Client: 100 User: BPINST Password: Welcome1 SID: S4H   For SAP HANA Studio:\n Host: vhcalhdbdb (local hosts file) Instance Number: 02 Multiple Containers - Tenant Database: HDB Port: 30215 User: SAPHANADB Password: The one used during the SAP deployment  If not existant:\n Right click on the left pane and select Add System  Fill the connection info  Fill the user and password   Once connection is created, double click on the connection on the left pane to see the HANA status: Alright! SAP has been deployed, we are able to connect, and we are ready to start installing the Azure required components on the next section.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/env_setup/azure_setup/adrole/",
	"title": "Assign Role to Application",
	"tags": [],
	"description": "",
	"content": " Navigate to the level of scope you wish to assign the application to. For example, to assign a role at the subscription scope, select All services, General and Subscriptions.\n Select the particular subscription to assign the application to. Select Access control (IAM) Select Add role assignment  Select the role you wish to assign to the application. To allow the application to execute actions like reboot, start and stop instances, select the Contributor role.  On the Members tab, click on Select members and start typing the name of the App registration done on the previous step Select Review and Assign to finish assigning the role.  You will see your application in the list of users assigned to a role for that scope.\nYour service principal is set up. You can start using it to run your scripts or apps. The next section shows how to get values that are needed when signing in programmatically.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/env_setup/azure_setup/getinfo/",
	"title": "Get required information",
	"tags": [],
	"description": "",
	"content": "In order to allow for SAP CAL to access your subscription, you will need some parameters. This section will explain where to find them.\nCopy those values to a temp text document so you can easily copy and paste later on.\nGo to your Azure Portal and follow the steps below:\n Go to Subscriptions and copy the Subscription ID  Select Azure Active Directory. Select Properties. Copy Tenant ID.  Still on Azure Active Directory, go to App registrations and select your application. Copy Application (client) ID.  Select Certificates \u0026amp; secrets, then select New client secret.  Define a name for the secret and an expiration 3 Months. After adding the client secret, the value of the client secret is displayed. Copy this value now because you aren\u0026rsquo;t able to retrieve the client secret later. You will provide the client secret value (as application password) with the application ID to sign in as the application. If you missed this step, delete the existing secret and create a new one.  By that time you should have 4 pieces of info:\n Azure Subscription ID Tenant ID Application (client) ID Secret Value  Now we will go to SAP CAL to setup the Azure access to deploy SAP S/4HANA in your subscription.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/",
	"title": "SAP on Microsoft Labs",
	"tags": [],
	"description": "",
	"content": "The following sections will be divided in different labs integrating several Microsoft services with SAP, such as:\n Automation and Integration  Azure Logic Apps Power Platform - Power Virtual Agents Power Platform - Power Automate Power Platform - Power Apps   Data \u0026amp; AI  Office 365 Integration Azure Synapse Power Platform - Power BI Azure Machine Learning   Security  Azure Sentinel Azure AD   Infra \u0026amp; Management  Azure Monitoring Azure Backup High-Availability    The following sections will guide you thru the steps to accomplish this and be ready to the labs.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/dataai/datafactory/step3/",
	"title": "SAP TABLE Provider",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s create a new pipeline and extract data to the Blob Storage:\n On the Data Factory Studio, click on the Pencil icon on the left, click the Dataset Ellipsis and select New Dataset  For the datastore, we will search for SAP and select SAP Table:  For the properties:   Name: SapTable1 Create a New under Linked Service   Fill the required data for the linked service:   Name: SapTable1 Integration Runtime: SAPIntegrationRuntime Server Name: \u0026laquo;HANA IP\u0026raquo; System Number: 00 Client Id: 100 Username: BPINST Password: Welcome1 Click on Test connection and if successful, click on Apply   Select the newly created SapTable1 linked service, select Table MATDOC and click OK   We are now communicating with SAP via Netweaver.\nJump to STEP 4 on this LAB: Extracting data (this is based on the values provided on the Step 3, but pipeline source data can be changed to match what the dataset we just set up)\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/inframgmt/backup/step2/",
	"title": "Run a Backup",
	"tags": [],
	"description": "",
	"content": "With Azure Backups configured and communicating with SAP HANA database, let\u0026rsquo;s execute a manual backup to test it.\nThis section will show the steps required for that to be accomplished:\n Log on to the Azure Portal and click on Backup items and then SAP HANA in Azure VM  You should see a screen similar to this, with a Warning (initial backup pending) message. Right+Click the elipsis and select Backup Now for both HDB and SYSTEMDB.  To monitor the jobs, click on Backup jobs and you will see both submitted jobs. They should be In Progress for now.  If you go to the Bastion Host and access HANA Studio, you can see the same jobs running from the SAP point of view. Double click Backup under HDB and see the right panel for details like estimated size, performance and status.  Double clicking on Backup for SYSTEMDB will bring another tab called Configuration. Here we can see the parameters set by the setup script we ran in the previous section. Backint is SAP\u0026rsquo;s backup agent for SAP, so having it configured means the agent is pushing data to Azure or other backup platform.  Wait until the backup is finished. In the example environment, it took 30 minutes to backup 251GB of data @ 142MB/s. Performance may vary depending on server usage and other conditions.  Same information can be seen on Azure Portal, by clicking on Backup Jobs and checking the status of the latest jobs:  By selecting Backup Items and checking the status SAP HANA Backups we will see SYSTEM and HDB. Click on View details for HDB.\n It will show the available point-in-time restore and backups made. Point-in-time restoers are a combination of Backups + Logs, allowing for the agent to reconstruct the database at a given point-in-time (In the picture below, we left it running during the night. In your environment you will see the green bar growing with time, depending of the Log Backups)   Congratulations, we are now ready to test the restore option. LEt\u0026rsquo;s move to the next section !\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/autoint/virtualagents/step3/",
	"title": "Publishing to Teams",
	"tags": [],
	"description": "",
	"content": "In this section we will publish our Chatbot to Teams and test it.\nOn the left side of the screen go to the Publish Publish the latest version. Once the bot is public, we will add it to our org\u0026rsquo;s Teams. Click on Go to Channels and then turn on Teams Channel After the publishing and channel activations succeeds, you should see the option to Open bot directly on Teams. On Teams, Add the Bot Run the same simulations as in the test bot. PO 0000000728 PO 0000001575 "
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/autoint/logicapps/step3/",
	"title": "Running the lab",
	"tags": [],
	"description": "",
	"content": "In this step we will execute the Workflow created previously\nGo back to Overview, Enable Debug mode (will be used in the future) and copy the Workflow URL provided We need to invoke the API, so we can use an externall tool like Postman or use the embeded Run Trigger tool.\nFor Postman:\n Method: POST URL: copied on the previous step Body: raw - JSON Body content: {\u0026quot;id\u0026quot;: \u0026quot;0000000728\u0026quot;} or {\u0026quot;id\u0026quot;: \u0026quot;0000001575\u0026quot;}  Note: SAP compares strings so have that in mind with leading zeroes and 10 total chars   Hit Send.   For Run Trigger tool:\n Click on Run Trigger with payload  Fill the request:  Method: POST Content-Type: application/json Body content: {\u0026quot;id\u0026quot;: \u0026quot;0000000728\u0026quot;} or {\u0026quot;id\u0026quot;: \u0026quot;0000001575\u0026quot;}  Note: SAP compares strings so have that in mind with leading zeroes and 10 total chars   Click Run    When your workflow runs, you should receive an Output status = 200  You can see the run status on the Run History tab  And by selecting the desired Run, you can see step-by-step inputs and outputs of your workflow for debug.  For a sucessful Run you can also see the details and the time it took on every step:   Now go check your email inbox because you should have something similar to the one below: If you need to check or compare a Sales Order on SAP, go to the Bastion Host via Remote Desktop, open SAP GUI and follow the steps below:\n Logon with BPINST/Welcome1  Go to TCODE VA03 (Display Sales Orders)\nSearch for the order number used on VA03\n Order: 728 or 1575 (here SAP compared numbers and don\u0026rsquo;t care about leading zeroes) Hit Search and you should see the details (to return to the previous screen, use the green arrow by the TCODE)   Congratulations ! You just finished the first lab and was able to create an API that will query SAP Sales Order and send an email, all with ZERO LINES of CODE !\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/autoint/automate/",
	"title": "Power Automate",
	"tags": [],
	"description": "",
	"content": "Power Automate What we will build   "
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": "Chapter X Some Chapter title Lorem Ipsum.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/env_setup/datagwsetup/",
	"title": "Microsoft On-premises Data Gateway Installation",
	"tags": [],
	"description": "",
	"content": "Once SAP is installed and access is working, we need to install the required components to allow for Microsoft services to interact with SAP S/4HANA environments.\nWhat you will need  Sucessfull deployment of S/4HANA and access ready  Expected duration This section is estimated to last no longer than 30 minutes\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/inframgmt/backup/step3/",
	"title": "Run a Restore",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s change some data and then run a restore for a point-in-time previous to the change.\nThis section will show the steps required for that to be accomplished:\nChanging some data  Go to the Bastion Host, and open the SAP GUI. Login with BPINST/Welcome1 default user.  On SAP GUI, go to TCODE MM02 to change a material.  For Material we will choose the CM-FL-V00 a Forklift. Click on the Checkmark.  For Plant we will choose the 1710 a Forklift. Click on the Checkmark.  Change the description from Forklift to something else. Click on SAVE   With the data changed, we now go to restore the database HDB to the previous state.\nRestoring the Database  Go to Backup items and select SAP HANA on Azure VM  Right+Click the ellipsis for HDB and select Restore  There will be 3 restore modes   Alternate Location - Restore to another HANA database on another host Overwrite DB - Restore on top of existing database Restore as Files - Restore to files on filesystem for manual recovery Select Overwrite DB and click on Select   Let\u0026rsquo;s pick 15-20 minutes prior to the time of change. In the example the change was 10:15, so we are selecting 9:59 as the point-in-time.  make sure the Restore Point is right and click OK  It will trigger a deployment.  To monitor the jobs, click on Backup jobs and you will see restore job. It should be In Progress for now. It should take roughly the same it took for backup. In this example 30 minutes.  In the meantime, if you check HANA Studio on BAstion host you will see HDB offline, being restored. REstore will take the database offline, restore it, and bring it back online.  After 30 minutes, the jobs is showing up as completed on the Azure Portal.   Now let\u0026rsquo;s check for the data restored\nChecking SAP status  Go to the Bastion Host and check HANA Studio for HDB being open. Double click for details.  Open SAP GUI. Login with BPINST/Welcome1 default user.  On SAP GUI, go to TCODE MM02 to change a material.  For Material we will choose the CM-FL-V00 a Forklift. Click on the Checkmark.  For Plant we will choose the 1710 a Forklift. Click on the Checkmark.  Description should be back to Forklift as it was before the change we did.   Congratulations, you have completed the Azure Backup for HANA Section !!!!\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/autoint/powerapps/",
	"title": "Power Apps",
	"tags": [],
	"description": "",
	"content": "Power Apps What we will build   "
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/inframgmt/",
	"title": "Infrastructure and Management",
	"tags": [],
	"description": "",
	"content": "Chapter X Some Chapter title Lorem Ipsum.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/labs/dataai/datafactory/step4/",
	"title": "Extracting data",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s add some steps on our pipeline to extract data to the Blob Storage:\n On the Data Factory Studio, click on the adfsap_pipeline on the left, open the Move \u0026amp; Transform activity, and drag Copy data to the main panel:  For the Copy data activity, we will name it Hana2Blob, under General tab:  On the Source tab we will select the dataset we want to read data from. In this example the HANA one, but it could be the SAP Table one as well.  For the Sink (destination) dataset, we will create a new one:  We will select Azure Blob Storage as the dataset type:  As Output format, we will select DelimitedText for the sake of visualization. For datalakes you will probably use other formats like Parquet that are more space efficient.  Name the dataset HANAMATDOCS and click on New under Linked Services.  We will link this dataset with our Storage Account created on step #1. Fill the required data for the linked service:   Name: AzureBlobSAP Integration Runtime: AutoResolveIntegrationRuntime Authentication Method: Account Key Subscription: Select your current subscription Storage Account Name: sapadfXXXX Click on Test connection and if successful, click on Apply   Back on the properties screen, make sure the Linked service is AzureBlobSAP and that filepath is cntsapadf which is the name of the container we created on step #1. You can use the folder button and navigate to the container itself. Make sure other options match the picture:  Our simple pipeline, with one step, is ready. Let\u0026rsquo;s Publish it so we can run it. On the top of the page, click on Publish and on the right panel Publish again:   Let\u0026rsquo;s run the Pipeline manually for testing:\n On the Data Factory Studio, click on the adfsap_pipeline on the left, and select AddTrigger/Trigger now on the main panel:  You can monitor the run, by selecting the speed meter icon on the left, then Pipeline runs and refreshing for Status = Succeedded   Let\u0026rsquo;s see the output !\n On the Azure Portal, go back to the sapadfXXXX Storage Account you created on step #1. There should be a container called cntadfsap. Click on it.  Inside the container we can see a newly created txt file with ~28MB, named after the table we used to extract the data. Right click it and select Download  Open the downloaded file on your editor of choice (aka Visual Code :) ) and check the file contents:  If you used the SAP TABLE provider (step 3), you should see a file named MATDOC.txt. Follow steps 2 \u0026amp; 3 and open this file instead.  Compare the extracted data with the content from HANA Studio, on the Bastion Host. Open HDB, go to Catalog, under SAPHANADB select Tables and run a query like \u0026lsquo;SELECT TOP 1000 FROM \u0026ldquo;SAPHANADB\u0026rdquo;.\u0026ldquo;MATDOC\u0026rdquo;\u0026rsquo;   Congratulations, you just learned how to integrate SAP with Azure Data Factory and include it in your pipelines.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/cleanup/",
	"title": "Environment Cleanup",
	"tags": [],
	"description": "",
	"content": "Chapter X Some Chapter title Lorem Ipsum.\n"
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://abicas.github.io/SapOnMicrosoftDemos/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]